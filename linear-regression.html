<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Meta -->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta charset="utf-8">

    <title>Linear Regression &ndash; Gastón Amengual</title>

    <!-- PWA -->
    <link rel="manifest" href="https://gastonamengual.github.io/theme/site.webmanifest">
    <meta name="theme-color" content="#6A1A6A">
    <link rel="apple-touch-icon" sizes="180x180" href="https://gastonamengual.github.io/theme/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://gastonamengual.github.io/theme/favicon/favicon-16x16.png">
    <link rel="shortcut icon" type="image/png" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">

    <!-- Social -->
    <meta property="article:author" content="Gastón Amengual" />
    <meta property="article:section" content="Machine Learning" />

    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Linear Regression"/>
    <meta property="og:description" content="Simple Linear Regression. Assumptions. Ordinary Least Squares. Implementation. Application and testing of assumptions. Confidence and prediction intervals. Multiple Linear Regression."/>
    <meta property="og:site_name" content="Gastón Amengual" />
    <meta property="og:url" content="https://gastonamengual.github.io/linear-regression.html"/>


    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Spartan:100,200,300,400,500,600,700,800,900">
    <link rel="stylesheet" type="text/css" href="https://gastonamengual.github.io/theme/style.css">
    <link href='fonts/fonts.css' rel='stylesheet'>
  </head>

  <body>
    <div id="navbar">
        <a href="../index.html"><img src="https://gastonamengual.github.io/theme/gaston_amengual.svg" id="navbar-logo"></a>
        <nav id="navbar-menu">
          <ul>
            
              <li class="navbar-li"><a href="/index.html">Home</a></li>
              <li class="navbar-li"><a href="/pages/about.html">About</a></li>
              <li class="navbar-li"><a href="/pages/portfolio.html">Portfolio</a></li>
              <li class="navbar-li"><a href="/categories.html">Articles</a></li>
          </ul>
        </nav>
    </div>



    <br><br>

    <article class="article-article">
      <header class="col-main article-header">
        <h1>Linear Regression</h1>
        <a class="article-category" href="https://gastonamengual.github.io/category/machine-learning.html">Machine Learning</a>
      </header>

      <div class="col-main">
        <section class="article-content">
          <h2>Table of Contents</h2>
<p><a href="#Simple-Linear-Regression">Simple Linear Regression</a></p>
<p><span class="math">\(\quad\)</span> <a href="#Assumptions">Assumptions</a></p>
<p><span class="math">\(\quad\)</span> <a href="#Ordinary-Least-Squares-Simple">Ordinary Least Squares Simple</a></p>
<p><span class="math">\(\quad\)</span> <a href="#SLR-Implementation">SLR-Implementation</a></p>
<p><span class="math">\(\quad\)</span> <a href="#SLR-Application">SLR-Application</a></p>
<p><span class="math">\(\quad\)</span> <a href="#Confidence-and-Prediction">Confidence and Prediction</a></p>
<p><br></p>
<p><a href="#Multiple-Linear-Regression">Multiple Linear Regression</a></p>
<p><span class="math">\(\quad\)</span> <a href="#Ordinary-Least-Squares-Multiple">Ordinary Least Squares Multiple</a></p>
<p><span class="math">\(\quad\)</span> <a href="#MLS-Implementation">MLS-Implementation</a></p>
<p><span class="math">\(\quad\)</span> <a href="#MLS-Application">MLS-Application</a></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">mse</span>
</code></pre></div>


<h1>Simple Linear Regression</h1>
<p>Linear regression is the simplest regression type. If the linear regression model has only one independent variable, then it is called Simple Linear Regression (SLR). When it has more than one independent variables, it is called Multiple Linear Regression. </p>
<p><br></p>
<p>The <strong>true SLR model</strong> describes the true relationship between independent and dependent variables, and it is achieved by carrying out the regression on the whole population, either with complete data or theoretical calculations. It is described by:</p>
<div class="math">$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$</div>
<p>where <span class="math">\(x_i\)</span> is the independent variable, <span class="math">\(y_i\)</span> is the true dependent variable, <span class="math">\((x_i, y_i)\)</span> constitute a true data pair or point, <span class="math">\(\beta_0\)</span> and <span class="math">\(\beta_1\)</span> are the true parameters, and <span class="math">\(\epsilon_i\)</span> is the error term. The <strong>errors</strong> are the deviations of the dependent variable observations from an unobservable function that relates the independent variable to the dependent variable.</p>
<p><br></p>
<p>However, in most cases the attributes of the whole population cannot be measured, so a sample is taken instead, and an <strong>estimated SLR model</strong> is constructed instead:</p>
<div class="math">$$\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i + r_i$$</div>
<p>where <span class="math">\(x_i\)</span> is the independent variable, <span class="math">\(\hat{y_i}\)</span> is the estimated dependent variable, <span class="math">\((x_i, \hat{y_i})\)</span> constitute a estimated data pair or point, <span class="math">\(\hat{\beta_0}\)</span> and <span class="math">\(\hat{\beta_1}\)</span> are the estimated parameters, and <span class="math">\(r_i\)</span> is the residual term. The <strong>residuals</strong> are the deviations of the dependent variable observations from the fitted function. The residuals are defined as:</p>
<div class="math">$$\hat{r_i} = \hat{y_i} - \hat{\beta_0} - \hat{\beta_1} x_i$$</div>
<p>Finally, the minimization problem, the <strong>sum of squared residuals</strong>, is:</p>
<div class="math">$$\min\limits_{\hat{\beta_0}, \hat{\beta_1}} Q(\hat{\beta_0}, \hat{\beta_1}) = \sum_{i=1}^{n}{\hat{r}_i^2 = \sum_{i=1}^{n}(\hat{y_i} - \hat{\beta_0} -\hat{\beta_1} x_i)^2}$$</div>
<p>As can be observed, the SLR model is a line, and the objective is to find the slope and intercept, i.e. the values of <span class="math">\(\hat{\beta_0}\)</span> and <span class="math">\(\hat{\beta_1}\)</span> that best fits the data points to the line, minimizing the difference of the true observed dependent variables and the estimated dependent variables. </p>
<h2>Assumptions</h2>
<p>To apply the Simple Linear Regression model (and more generally the Linear Regression model) to a data set, it must meet a set of assumptions. If they are not met, then the results of the model can be mistaken or not precise.  </p>
<h3>Linearity</h3>
<p>There is a linear relationship between the independent variable <span class="math">\(x\)</span> and the dependent variable <span class="math">\(y\)</span>, that is, the expected value of the dependent variable is a straight-line function of the independent variable. </p>
<p><br></p>
<p><strong>How to test?</strong></p>
<p>Linearity is usually evident in a plot of the independent variable versus the dependent variable (1), or in a plot of residuals versus the independent variable. In (1) the points should be symmetrically distributed around a diagonal line, while in (2) they should be distributed around a horizontal line, with a roughly constant variance. The latter approach is better, as it removes the visual distraction of a sloping pattern.</p>
<p><br></p>
<p><strong>What if the model is nonlinear?</strong></p>
<p>If the model is nonlinear, the predictions made are likely to be mistaken, especially when extrapolating beyond the range of the sample data. Several transformations can be applied nonlinear data (see <a href="https://people.revoledu.com/kardi/tutorial/Regression/nonlinear/NonLinearTransformation.htm">here</a> for more details), or another independent variable can be added.</p>
<h3>Independence</h3>
<p>The residuals must be independent.</p>
<p><br>
<strong>How to test?</strong></p>
<p>Independence can be tested with an autocorrelation plot of the residuals. Ideally, most of the residuals should fall within the 95% confidence bands around zero, where <span class="math">\(n\)</span> is the sample size. For a more formal and precise test, the <a href="https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic">Durbin-Watson Test</a> can be conducted.</p>
<p><br>
<strong>What if the residuals are not independent?</strong></p>
<p>Violations of independence are potentially very serious in time series regression models. To fix the issue, there are several options. For positive serial correlation, add lags of the dependent and/or independent variable to the model. For negative serial correlation, check to make sure that none of the variables are overdifferenced. For seasonal correlation, add seasonal dummy variables to the model.</p>
<h3>Homoscedasticity</h3>
<p>The residuals must have constant variance at every level of the independent variable.</p>
<p><br>
<strong>How to test?</strong></p>
<p>Formal tests of heteroscedasticity can be studied <a href="https://medium.com/@remycanario17/tests-for-heteroskedasticity-in-python-208a0fdb04ab">here</a>. However, for a simpler way to estimate whether the residuals are heteroscedastic, apply resampling to a partition of continuous values of the residuals, and then construct confidence intervals for each of them. If all such CIs overlap with each other, then it is probable that the residuals are not heteroscedastic.</p>
<p><br>
<strong>What if the residuals are not independent?</strong></p>
<p>Transform the dependent variable, e.g., the log of the dependent variable, redefine the dependent variable, e.g., use a rate rather than the raw value, or use weighted regression.</p>
<h3>Normality</h3>
<p>The residuals must be normally distributed with mean <span class="math">\(0\)</span>.</p>
<p><br>
<strong>How to test?</strong></p>
<p>The normality can be checked using statistical tests like Shapiro-Wilk, Kolmogorov-Smironov, Jarque-Barre, or D’Agostino-Pearson. However, they are sensitive to large sample sizes, as they often conclude that the residuals are not normal when the sample size is large. The value of the mean can be tested with the T-test for the mean of ONE group of scores.</p>
<p><br>
<strong>What if the residuals are not independent?</strong></p>
<p>Violations in normality create problems for determining whether model coefficients are significantly different from zero and for calculating confidence intervals for forecasts. Sometimes the residuals distribution is "skewed" by the presence of a few large outliers. To correct this problem, verify that outliers do not have great impact on the distribution, and apply a nonlinear transformation to the independent and/or dependent variable.</p>
<h2>Ordinary Least Squares Simple</h2>
<p>Ordinary Least Squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model.</p>
<p>The sum of squared residuals function is defined as</p>
<div class="math">$$Q(\hat{\beta_0}, \hat{\beta_1}) = \sum_{i=1}^{n}(y_{i}-\hat{\beta_0} -\hat{\beta_1} x_{i})^2$$</div>
<p>The values of <span class="math">\(\hat{\beta_0}\)</span> and <span class="math">\(\hat{\beta_1}\)</span> that minimizes the function are found as follows:</p>
<p><strong>1. Take the partial derivatives with respect to <span class="math">\(\hat{\beta_0}\)</span> and <span class="math">\(\hat{\beta_1}\)</span>.</strong></p>
<p><span class="math">\(\frac{\partial }{\partial \hat{\beta_0}}\sum_{i=1}^{n}(\hat{y_i}-\hat{\beta_0} -\hat{\beta_1} x_i)^2 = -2 \sum_{i=1}^{n}(\hat{y_i}-\hat{\beta_0} -\hat{\beta_1} x_i)\)</span></p>
<p><span class="math">\(\frac{\partial }{\partial \hat{\beta_1}}\sum_{i=1}^{n}(\hat{y_i}-\hat{\beta_0} -\hat{\beta_1} x_i)^2 = -2 \sum_{i=1}^{n} x_i(\hat{y_i}-\hat{\beta_0} -\hat{\beta_1} x_i)\)</span></p>
<p><br></p>
<p><strong>2. Set the partial derivatives equal to 0.</strong></p>
<p><span class="math">\(\sum_{i=1}^{n}(\hat{y_i}-\hat{\beta_0} -\hat{\beta_1} x_i) = 0 \quad (1)\)</span></p>
<p><span class="math">\(\sum_{i=1}^{n} x_i(\hat{y_i}-\hat{\beta_0} -\hat{\beta_1} x_i) = 0 \quad (2)\)</span></p>
<p>(Note that <span class="math">\(-2\)</span> can be simplified)</p>
<p><br></p>
<p><strong>3. Solve for  <span class="math">\(\hat{\beta_0}\)</span> and <span class="math">\(\hat{\beta_1}\)</span>.</strong></p>
<p><em>Solving for <span class="math">\(\hat{\beta_0}\)</span> in <span class="math">\((1)\)</span>,</em></p>
<p><span class="math">\(\sum_{i=1}^{n}(\hat{y_i}-\hat{\beta_0} -\hat{\beta_1} x_i) = 0\)</span></p>
<p><span class="math">\(\sum_{i=1}^{n} \hat{y_i} - \sum_{i=1}^{n} \hat{\beta_0} - \sum_{i=1}^{n} \hat{\beta_1} x_i = 0\)</span></p>
<p><span class="math">\(\sum_{i=1}^{n} \hat{y_i} - n \hat{\beta_0} - \hat{\beta_1} \sum_{i=1}^{n} x_i = 0\)</span></p>
<p><span class="math">\(\hat{\beta_0} = \dfrac{\sum_{i=1}^{n} \hat{y_i}}{n} - \dfrac{\hat{\beta_1} \sum_{i=1}^{n} x_i}{n}\)</span></p>
<div class="math">$$\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}$$</div>
<p><em>Replacing <span class="math">\(\hat{\beta_0}\)</span> in <span class="math">\((2)\)</span> and solving for <span class="math">\(\hat{\beta_1}\)</span>,</em></p>
<p><span class="math">\(\sum_{i=1}^{n} x_i(\hat{y_i}- \bar{y} + \hat{\beta_1}( \bar{x} - x_i)) = 0\)</span></p>
<p><span class="math">\(\sum_{i=1}^{n} x_i(\hat{y_i}- \bar{y}) = \hat{\beta_1} \sum_{i=1}^{n} x_i (\bar{x} - x_i)\)</span></p>
<p><span class="math">\(\hat{\beta_1} = \dfrac{\sum_{i=1}^{n} x_i (\hat{y_i} - \bar{y})}{\sum_{i=1}^{n} x_i (x_i - \bar{x})}\)</span></p>
<p>It can be further demonstrated that the expression above equals</p>
<div class="math">$$\hat{\beta_1} = \dfrac{\sum_{i=1}^{n} (x_i - \bar{x}) (\hat{y_i} - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$$</div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">ordinary_least_squares_simple</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">beta_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x_</span> <span class="o">-</span> <span class="n">x_</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span> <span class="o">/</span> <span class="p">((</span><span class="n">x_</span> <span class="o">-</span> <span class="n">x_</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">beta_0</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">x_</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span>
</code></pre></div>


<h2>SLR Implementation</h2>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">simple_liner_regression_fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span> <span class="o">=</span> <span class="n">ordinary_least_squares_simple</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">simple_linear_regression_predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">beta_1</span>
    <span class="k">return</span> <span class="n">predictions</span>
</code></pre></div>


<h2>SLR Application</h2>
<p><strong>Data set</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div>


<p><strong>Train</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span> <span class="o">=</span> <span class="n">simple_liner_regression_fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;β0: </span><span class="si">{</span><span class="n">beta_0</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;β1: </span><span class="si">{</span><span class="n">beta_1</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="c">β0: -0.3420</span>
<span class="c">β1: 10.2998</span>
</code></pre></div>


<p><strong>Test</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">y_estimated</span> <span class="o">=</span> <span class="n">simple_linear_regression_predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_estimated</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;RMSE: </span><span class="si">{</span><span class="n">rmse</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">RMSE</span><span class="o">:</span> <span class="mf">20.4741</span>
</code></pre></div>


<p><strong>Line Prediction</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span> <span class="o">=</span> <span class="n">simple_liner_regression_fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_estimated</span> <span class="o">=</span> <span class="n">simple_linear_regression_predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_estimated</span><span class="p">)</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/linear_regression_1.png"></p>
<p>The SLR model is given then by the line given by the function</p>
<div class="math">$$\hat{y_i} = -0.21 + 10.77 \; x_i + r_i$$</div>
<p>To know for certain that this model is accurate and precise, the four assumptions will be verified.</p>
<div class="highlight"><pre><span></span><code><span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_estimated</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</code></pre></div>


<p><br></p>
<p><strong>Assumption 1: Linearity</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/linear_regression_2.png"></p>
<p>A clear linear relationship can be observed between <span class="math">\(x\)</span> and <span class="math">\(y\)</span>, while most of the residuals are distributed around a horizontal line, with a roughly constant variance. </p>
<p><br></p>
<p><strong>Assumption 2: Independence of errors</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">pd</span><span class="o">.</span><span class="n">plotting</span><span class="o">.</span><span class="n">autocorrelation_plot</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/linear_regression_3.png">    </p>
<p>It can be seen that the autocorrelation falls between the confidence interval, and I don't know what that means, and what CI we're talking about...</p>
<p><br></p>
<p><strong>Assumption 3: Homoscedasticity</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">n</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">sets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">residuals</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>

    <span class="n">variances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="n">resample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">residuals</span><span class="p">[</span><span class="n">sets</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">sets</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]],</span> <span class="n">sets</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">variances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">resample</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;CI for Sample </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">variances</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> - </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">variances</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">variances</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="err">CI for Sample 1: 382.43 - 438.31</span>
<span class="err">CI for Sample 2: 377.82 - 436.26</span>
<span class="err">CI for Sample 3: 365.82 - 418.22</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/linear_regression_4.png"></p>
<p>It can be seen the the three histograms overlap in their CI, show that there is a high probability that the residuals are not heteroscedastic. For the purposes of this work, this conclusion is enough to consider the residuals homoscedastic, while formal testing is encouraged.</p>
<p><br></p>
<p><strong>Assumption 4: Normality</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">_</span><span class="p">,</span> <span class="n">pvalue</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">ttest_1samp</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">popmean</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">if</span> <span class="n">pvalue</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Null Hypothesis: &quot;The CI for the residual population mean contains 0&quot; rejected.&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Null Hypothesis: &quot;The CI for the residual population mean contains 0&quot; not rejected.&#39;</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">pvalue</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">normaltest</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>

<span class="k">if</span> <span class="n">pvalue</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Null Hypothesis: &quot;the residuals sample comes from a normal distribution&quot; rejected.&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Null Hypothesis: &quot;the residuals sample comes from a normal distribution&quot; not rejected.&#39;</span><span class="p">)</span>
</code></pre></div>


<p>Null Hypothesis: "The CI for the residual population mean contains 0" not rejected.</p>
<p>Null Hypothesis: "the residuals sample comes from a normal distribution" not rejected.</p>
<p><br></p>
<p>Both Null Hypothesis were not rejected, and it can be concluded that the residuals follow a normal distribution with mean <span class="math">\(0\)</span>.</p>
<p><br></p>
<p><strong>In conclusion, the data set meets the four SLR assumptions.</strong></p>
<h2>Confidence and Prediction</h2>
<p>Suppose a new value of <span class="math">\(x\)</span>, <span class="math">\(x_{new}\)</span>. There are two questions that can be asked:</p>
<ul>
<li>
<p>Which is the most probable value (mean response) for a new observation <span class="math">\(x_{new}\)</span>?</p>
</li>
<li>
<p>What will be the range of possible response values for <span class="math">\(y_{new}\)</span> for a new observation <span class="math">\(x_{new}\)</span>?</p>
</li>
</ul>
<p><span class="math">\(\hat{y} = \beta_0 + \beta_1 x_{new}\)</span> is the best answer to the first question, whereas to answer the second question, a confidence interval for <span class="math">\(y_{new}\)</span> must be built.</p>
<h3>Confidence interval for mean response</h3>
<p>It is also called a <em>t</em>-interval. The general formula is</p>
<div class="math">$$\text{Sample estimate} \pm (\text{t-multiplier} \times \text{standard error})$$</div>
<p>Mathematically, </p>
<div class="math">$$\hat{y}_h \pm t_{(\alpha/2, n-2)} \times \sqrt{MSE \times \left( \frac{1}{n} + \dfrac{(x_{new}-\bar{x})^2}{\sum(x_i-\bar{x})^2}\right)}$$</div>
<p>where:</p>
<ul>
<li>
<p><span class="math">\(\hat{y}_h\)</span> is the fitted or predicted value of the response when the predictor is <span class="math">\(x_h\)</span>. </p>
</li>
<li>
<p><span class="math">\(t_{(\alpha/2, n-2)}\)</span> is the "t-multiplier." (with n-2 degrees of freedom).</p>
</li>
<li>
<p><span class="math">\(\sqrt{MSE \times \left( \frac{1}{n} + \frac{(x_h-\bar{x})^2}{\sum(x_i-\bar{x})^2}\right)}\)</span> is the "standard error of the fit," which depends on the mean square error (MSE), the sample size (n), how far in squared units the predictor value <span class="math">\(x_h\)</span> is from the average of the predictor values <span class="math">\(\bar{x}\)</span>, or <span class="math">\((x_h-\bar{x})^2\)</span>, and the sum of the squared distances of the predictor values <span class="math">\(x_i\)</span> from the average of the predictor values <span class="math">\(\bar{x}\)</span>, or <span class="math">\(\sum(x_i-\bar{x})^2\)</span>.</p>
</li>
</ul>
<p>From the formula above, it can be concluded that:</p>
<ul>
<li>As the MSE decreases, the width of the interval decreases.</li>
<li>As the sample size increases, the width of the interval decreases. </li>
<li>The more spread out the predictor values, the narrower the interval.</li>
<li>The closer <span class="math">\(x_h\)</span> is to the average of the sample's predictor values, the narrower the interval.</li>
</ul>
<h3>Prediction interval for <span class="math">\(y_{new}\)</span></h3>
<p>It is similar to the formula explained above, except that an extra MSE term is added:</p>
<div class="math">$$\hat{y}_h \pm t_{(\alpha/2, n-2)} \times \sqrt{MSE \times \left(1+ \frac{1}{n} + \dfrac{(x_{new}-\bar{x})^2}{\sum(x_i-\bar{x})^2}\right)}$$</div>
<p>It must be noticed that, because of the extra MSE term, a confidence interval for <span class="math">\(y_{new}\)</span> at <span class="math">\(x_h\)</span> will always be wider than the corresponding confidence interval for <span class="math">\(\mu_y\)</span> at <span class="math">\(x_h\)</span>.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mean_response_interval</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">residuals</span><span class="p">):</span>

    <span class="c1"># 95% Confidence: 0.025</span>
    <span class="n">t_multiplier</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>

    <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">x_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">mean_margin</span> <span class="o">=</span> <span class="n">t_multiplier</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">x_size</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_new</span> <span class="o">-</span> <span class="n">x_mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">x_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))))</span>

    <span class="k">return</span> <span class="n">mean_margin</span>

<span class="k">def</span> <span class="nf">prediction_interval</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">residuals</span><span class="p">):</span>

    <span class="c1"># 95% Confidence: 0.025</span>
    <span class="n">t_multiplier</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>

    <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">x_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">prediction_margin</span> <span class="o">=</span> <span class="n">t_multiplier</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">x_size</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_new</span> <span class="o">-</span> <span class="n">x_mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">x_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))))</span>

    <span class="k">return</span> <span class="n">prediction_margin</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">x_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y_estimated_new</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">x_new</span> <span class="o">*</span> <span class="n">beta_1</span>

<span class="n">mean_margin</span> <span class="o">=</span> <span class="n">mean_response_interval</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>
<span class="n">prediction_margin</span> <span class="o">=</span> <span class="n">prediction_interval</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>

<span class="c1"># Confidence Interval</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">y_estimated_new</span> <span class="o">+</span> <span class="n">mean_margin</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">y_estimated_new</span> <span class="o">-</span> <span class="n">mean_margin</span><span class="p">,)</span>

<span class="c1"># Prediction Interval</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">y_estimated_new</span> <span class="o">+</span> <span class="n">prediction_margin</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">y_estimated_new</span> <span class="o">-</span> <span class="n">prediction_margin</span><span class="p">)</span>

<span class="c1"># Data Points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Fitted Line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">y_estimated_new</span><span class="p">)</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/linear_regression_5.png"></p>
<div class="highlight"><pre><span></span><code><span class="n">x_new_single</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">y_predicted_x_new</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">x_new_single</span>
<span class="n">prediction_margin</span> <span class="o">=</span> <span class="n">prediction_interval</span><span class="p">(</span><span class="n">x_new_single</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prediction for </span><span class="si">{</span><span class="n">x_new_single</span><span class="si">}</span><span class="s2"> =&gt; </span><span class="si">{</span><span class="n">y_predicted_x_new</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">prediction_margin</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">x_new_single</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">y_predicted_x_new</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">x_new_single</span>
<span class="n">prediction_margin</span> <span class="o">=</span> <span class="n">prediction_interval</span><span class="p">(</span><span class="n">x_new_single</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prediction for </span><span class="si">{</span><span class="n">x_new_single</span><span class="si">}</span><span class="s2"> =&gt; </span><span class="si">{</span><span class="n">y_predicted_x_new</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">prediction_margin</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">x_new_single</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">y_predicted_x_new</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">x_new_single</span>
<span class="n">prediction_margin</span> <span class="o">=</span> <span class="n">prediction_interval</span><span class="p">(</span><span class="n">x_new_single</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prediction for </span><span class="si">{</span><span class="n">x_new_single</span><span class="si">}</span><span class="s2"> =&gt; </span><span class="si">{</span><span class="n">y_predicted_x_new</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">prediction_margin</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI for Residuals: </span><span class="si">{</span><span class="n">residuals</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">residuals</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">residuals</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>


<p>Prediction for 1 =&gt; 10.56 ± 56.52</p>
<p>Prediction for 10 =&gt; 107.54 ± 402.09</p>
<p>Prediction for 30 =&gt; 323.07 ± 1199.95</p>
<p>95% CI for Residuals: -0.00 ± 40.16</p>
<p><br></p>
<p>From the previous analysis, it can be concluded that any prediction constructed must be reported as a value with a confidence interval, rather than as a single number. Moreover, as the value of <span class="math">\(x_{new}\)</span> moves away from the center of the explanatory variables (in this case the mean), the prediction interval grows wider, making the model not suitable for extrapolation, but rather for interpolation (with the corresponding CI). Then, interpolated values are much more reliable than are extrapolated values.</p>
<h1>Multiple Linear Regression</h1>
<p>A Multiple Linear Regression (MLR) is a linear approach to modeling the relationship between a dependent variable <span class="math">\(y\)</span> and multiple independent variables <span class="math">\(X\)</span>.</p>
<p>Let <span class="math">\(X\)</span> be an <span class="math">\(n \times m\)</span> matrix, where <span class="math">\(n\)</span> is the number of observations and <span class="math">\(m\)</span> is the number of features, and <span class="math">\(y\)</span> be an <span class="math">\(n \times 1\)</span> vector, the linear regression model assumes a linear relationship between <span class="math">\(X\)</span> and <span class="math">\(y\)</span>. Mathematically, this relationship is modeled as</p>
<div class="math">$$y_i = \beta_0 + \beta_1 x_{i1} + \dotsc + \beta_m x_{im} + \epsilon_i$$</div>
<p>where <span class="math">\(\beta_0\)</span> is the intercept, <span class="math">\(\beta_1, \cdots, \beta_m\)</span> are the coefficients, and <span class="math">\(\epsilon\)</span> is the error, an unobserved random variable that adds noise to the linear relationship between the dependent and the independent variables.</p>
<p>In matrix notation, the model is described by</p>
<div class="math">$$Y = X^T \beta + \epsilon$$</div>
<p>where</p>
<div class="math">$$Y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix} \qquad X= \begin{pmatrix} x_1^T \\ x_2^T \\\vdots \\ x_n^T \end{pmatrix} = \begin{pmatrix} 1 &amp; x_{11} &amp; \cdots &amp; x_{1m} \\ 1 &amp; x_{21} &amp; \cdots &amp; x_{2m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; \cdots &amp; x_{nm} \end{pmatrix} \qquad \beta = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_m \end{pmatrix} \qquad \epsilon = \begin{pmatrix} \epsilon_1 \\ \epsilon_{2} \\ \vdots \\ \epsilon_n \end{pmatrix}$$</div>
<p>It can be observed that an extra column of <span class="math">\(1\)</span>s is added to <span class="math">\(X\)</span>, to vouch for the intercept coefficient.</p>
<h2>Ordinary Least Squares Multiple</h2>
<p>The Ordinary Least Squares for multiple variables can be written as</p>
<div class="math">$$\hat{\beta} = \left( X^T X \right)^{-1} X^Ty$$</div>
<p>For a full demonstration, relate to this <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Matrix/vector_formulation">website</a>.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">ols_multiple</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">intercept_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">X_with_intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">intercept_matrix</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
    <span class="n">betas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_with_intercept</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_with_intercept</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">coefficients</span> <span class="o">=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">coefficients</span>
</code></pre></div>


<h2>MLS Implementation</h2>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">multiple_liner_regression_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">intercept</span><span class="p">,</span> <span class="n">coefficients</span> <span class="o">=</span> <span class="n">ols_multiple</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">coefficients</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">multiple_linear_regression_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">coefficients</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">intercept</span> <span class="o">+</span> <span class="n">X</span> <span class="o">@</span> <span class="n">coefficients</span>
    <span class="k">return</span> <span class="n">predictions</span>
</code></pre></div>


<h2>MLS Application</h2>
<p><strong>Data set</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">boston</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_boston</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div>


<p><strong>Train</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">intercept</span><span class="p">,</span> <span class="n">coefficients</span> <span class="o">=</span> <span class="n">multiple_liner_regression_fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;β0: </span><span class="si">{</span><span class="n">intercept</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">coefficients</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;β</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">c</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>


<p><span class="math">\(\beta_0: 30.247\)</span></p>
<p><span class="math">\(\beta_1: -0.113\)</span></p>
<p><span class="math">\(\beta_2: 0.030\)</span></p>
<p><span class="math">\(\beta_3: 0.040\)</span></p>
<p><span class="math">\(\beta_4: 2.784\)</span></p>
<p><span class="math">\(\beta_5: -17.203\)</span></p>
<p><span class="math">\(\beta_6: 4.439\)</span></p>
<p><span class="math">\(\beta_7: -0.006\)</span></p>
<p><span class="math">\(\beta_8: -1.448\)</span></p>
<p><span class="math">\(\beta_9: 0.262\)</span></p>
<p><span class="math">\(\beta_10: -0.011\)</span></p>
<p><span class="math">\(\beta_11: -0.915\)</span></p>
<p><span class="math">\(\beta_12: 0.012\)</span></p>
<p><span class="math">\(\beta_13: -0.509\)</span></p>
<p><br></p>
<p><strong>Test</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">y_estimated</span> <span class="o">=</span> <span class="n">multiple_linear_regression_predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">coefficients</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_estimated</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>


<p>RMSE: 4.9286</p>
<p><br></p>
<h1>References</h1>
<p><a href="https://en.wikipedia.org/wiki/Linear_regression" target="_blank">Linear Regression - Wikipedia</a></p>
<p><a href="https://en.wikipedia.org/wiki/Simple_linear_regression" target="_blank">Simple Linear Regression - Wikipedia</a></p>
<p><a href="https://www.youtube.com/watch?v=ewnc1cXJmGA" target="_blank">Deriving the least squares estimators - jbstatistics</a></p>
<p><a href="https://www.statology.org/linear-regression-assumptions/" target="_blank">The four assumptions of linear regression - statology</a></p>
<p><a href="http://people.duke.edu/~rnau/testing.htm" target="_blank">Testing the assumptions of linear regression - people.duke.edu</a></p>
<p><a href="https://stats.stackexchange.com/questions/219579/what-is-wrong-with-extrapolation" target="_blank">What is wrong with extrapolation? - stats.stackexchange</a></p>
<p><a href="http://www2.stat.duke.edu/~tjl13/s101/slides/unit6lec3H.pdf" target="_blank">Confidence and prediction intervals for SLR - stat.duke.edu</a></p>
<p><a href="https://online.stat.psu.edu/stat501/lesson/3/3.3" target="_blank">Prediction Interval for a New Response - online.stat.psu.edu</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </section>
      </div>

      <br><br>

    </article>


    <footer>
        <p>
          &copy;
          2021          Gastón Amengual
        </p>
    </footer>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-6KC62F9717"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());
  gtag('config', "G-6KC62F9717");</script>  </body>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: { inlineMath: [["$","$"],["\\(","\\)"]] },
    "HTML-CSS": {
      linebreaks: { automatic: true, width: "container" }
    }
    });
  </script>
</html>