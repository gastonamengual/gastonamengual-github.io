<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Meta -->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta charset="utf-8">

    <title>k-Means &ndash; Gast贸n Amengual</title>

    <!-- PWA -->
    <link rel="manifest" href="https://gastonamengual.github.io/theme/site.webmanifest">
    <meta name="theme-color" content="#6A1A6A">
    <link rel="apple-touch-icon" sizes="180x180" href="https://gastonamengual.github.io/theme/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://gastonamengual.github.io/theme/favicon/favicon-16x16.png">
    <link rel="shortcut icon" type="image/png" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">

    <!-- Social -->
    <meta property="article:author" content="Gast贸n Amengual" />
    <meta property="article:section" content="Machine Learning" />

    <meta property="og:type" content="article"/>
    <meta property="og:title" content="k-Means"/>
    <meta property="og:description" content="Definition. Assumptions. Steps. From-Scrath Python implementation. Application on dummy data."/>
    <meta property="og:site_name" content="Gast贸n Amengual" />
    <meta property="og:url" content="https://gastonamengual.github.io/k-means.html"/>


    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Spartan:100,200,300,400,500,600,700,800,900">
    <link rel="stylesheet" type="text/css" href="https://gastonamengual.github.io/theme/style.css">
    <link href='fonts/fonts.css' rel='stylesheet'>
  </head>

  <body>
    <div id="navbar">
        <a href="../index.html"><img src="https://gastonamengual.github.io/theme/gaston_amengual.svg" id="navbar-logo"></a>
        <nav id="navbar-menu">
          <ul>
            
              <li class="navbar-li"><a href="/index.html">Home</a></li>
              <li class="navbar-li"><a href="/pages/about.html">About</a></li>
              <li class="navbar-li"><a href="/pages/portfolio.html">Portfolio</a></li>
              <li class="navbar-li"><a href="/categories.html">Articles</a></li>
          </ul>
        </nav>
    </div>



    <br><br>

    <article class="article-article">
      <header class="col-main article-header">
        <h1><em>k</em>-Means</h1>
        <a class="article-category" href="https://gastonamengual.github.io/category/machine-learning.html">Machine Learning</a>
      </header>

      <div class="col-main">
        <section class="article-content">
          <div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="kn">import</span> <span class="n">distance_matrix</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">pdist</span>
</code></pre></div>


<h1>Definition</h1>
<p><em>k</em>-means is a method of vector quantization (originally from signal processing) that aims to partition <span class="math">\(n\)</span> observations into <span class="math">\(k\)</span> clusters, in which each observation belongs to the cluster with the nearest mean (cluster centers or centroids). It is however most commonly defined as an unsupervised machine learning algorithm for clustering. </p>
<h3>Characteristics</h3>
<p>As a partitional clustering algorithm, it has the following characteristics:</p>
<ul>
<li>It divides data objects into non-overlapping groups: no object can be a member of more than one cluster.</li>
<li>Every cluster must have at least one object.</li>
<li>It finds a grouping so that similar objects are in the same cluster and dissimilar objects are in different clusters.</li>
</ul>
<h3>Assumptions</h3>
<p><em>k</em>-means should be applied to data sets that meet the following conditions:</p>
<ul>
<li>Clusters are spatially grouped, convex, spherical, and isotropic.</li>
<li>Clusters are of a similar size and density.</li>
<li>There are no outliers.</li>
</ul>
<h3>Model</h3>
<p>Let X be an <span class="math">\(n \times m\)</span> matrix, with <span class="math">\(n\)</span> observations vectors of <span class="math">\(m\)</span> features. The algorithm divides a <span class="math">\(X\)</span> into <span class="math">\(k\)</span> disjoint clusters <span class="math">\(C_j\)</span> <span class="math">\((j=1,2,\dotsc,k)\)</span>, each described by the mean or centroid <span class="math">\(\mu_j\)</span> of the samples in the cluster. These centroids are chosen so that they minimize the <strong>within-cluster sum-of-squares (WCSS)</strong> criterion. Namely,</p>
<div class="math">$$\sum_{i=1}^{n} \underset{\mu_j \in C_j}{\min} = (||x_i - \mu_{j}||)^2$$</div>
<p>The optimization problem then consists on minimizing the sum of the norm (distance) of the vector that results from the difference of each centroid and its its assigned samples. </p>
<h3>Steps</h3>
<p>The most basic <em>k</em>-means algorithm is called Lloyd's algorithm. It requires a number <span class="math">\(k\)</span> of clusters to assign as a parameter, and it has the following steps:</p>
<h3></h3>
<p><span class="math">\(\quad\)</span> <strong>1.</strong> Randomly initialize <span class="math">\(k\)</span> centroids, either as <span class="math">\(k\)</span> random samples from the data, or as <span class="math">\(k\)</span> random points in the domain of the data.</p>
<h3></h3>
<p><em>repeat</em></p>
<h3></h3>
<p><span class="math">\(\quad\)</span> <strong>2.</strong> Assign each point to its nearest centroid.</p>
<p><span class="math">\(\quad\)</span> <strong>3.</strong> Create the new centroids as the mean of the samples assigned to each previous centroid.</p>
<p><span class="math">\(\quad\)</span> <strong>4.</strong> Calculate the difference between the old and the new centroids.</p>
<h3></h3>
<p><em>until convergence</em>.</p>
<h3>Initialization</h3>
<p>Given enough time, K-means will always converge. However, depending on the initialization of the centroids, this may be to a local minimum. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue consists on initializing the centroids to be (generally) distant from each other, leading to provably better results than random initialization.</p>
<h3>Convergence</h3>
<p>Convergence is reached when the difference between old and new centroids is less than a threshold <span class="math">\(\epsilon\)</span>, meaning that the centroids do not move significantly.</p>
<h1>Implementation</h1>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">k_means</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>

    <span class="c1"># Step 1 - Initialize centroids as random sample points</span>
    <span class="n">centroids</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">k</span><span class="p">)]</span>

    <span class="n">centroids_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">centroids</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>

        <span class="c1"># Step 2</span>
        <span class="c1"># Calculate distances from each point to all centroids</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">distance_matrix</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">centroids</span><span class="p">)</span>

        <span class="c1"># Calculate nearest centroid to each point</span>
        <span class="n">estimations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">previous_centroids</span> <span class="o">=</span> <span class="n">centroids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># Step 3 - Calculate new centroid as mean of samples</span>
        <span class="n">centroids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">estimations</span> <span class="o">==</span> <span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">)])</span>
        <span class="n">centroids_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">centroids</span><span class="p">)</span>

        <span class="c1"># Step 4 - Check for convergence</span>
        <span class="n">difference_between_previous_absolute</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">centroids</span> <span class="o">-</span> <span class="n">previous_centroids</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">difference_between_previous_relative</span> <span class="o">=</span> <span class="n">difference_between_previous_absolute</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">previous_centroids</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">difference_between_previous_relative</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">estimations</span><span class="p">,</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">centroids_list</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">calculate_wcss</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">estimations</span><span class="p">,</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">wcss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">estimations</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">norm_squared</span> <span class="o">=</span> <span class="n">norm</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">wcss</span> <span class="o">+=</span> <span class="n">norm_squared</span>
    <span class="k">return</span> <span class="n">wcss</span>
</code></pre></div>


<h1>Application</h1>
<h3>Create Dummy Data Set</h3>
<div class="highlight"><pre><span></span><code><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/k_means_1.png"></p>
<h3>Find optimum <em>k</em></h3>
<p>In order to choose the optimum value of clusters, the WCSS will be calculated for different values of <em>k</em>.</p>
<div class="highlight"><pre><span></span><code><span class="n">ks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">wcss_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ks</span><span class="p">:</span>
    <span class="n">estimations</span><span class="p">,</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">centroids_list</span> <span class="o">=</span> <span class="n">k_means</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">wcss</span> <span class="o">=</span> <span class="n">calculate_wcss</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">estimations</span><span class="p">,</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">wcss_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wcss</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">wcss_list</span><span class="p">)</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/k_means_2.png"></p>
<p>From applying the elbow method, it can be observed that <span class="math">\(k=3\)</span> results in a lower WCSS than <span class="math">\(k=1\)</span> and <span class="math">\(k=2\)</span>, and increasing the number of clusters from <span class="math">\(k=3\)</span> does not improve the WCSS significantly, for what it is not necessary to add more clusters. It is concluded than the optimum value of clusters is <span class="math">\(k=3\)</span>. </p>
<h3>Run <em>k</em>-means with optimum <em>k</em></h3>
<div class="highlight"><pre><span></span><code><span class="n">optimum_k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">estimations</span><span class="p">,</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">centroids_list</span> <span class="o">=</span> <span class="n">k_means</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">optimum_k</span><span class="p">)</span>
<span class="n">wcss</span> <span class="o">=</span> <span class="n">calculate_wcss</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">estimations</span><span class="p">,</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">optimum_k</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;darkmagenta&quot;</span><span class="p">,</span> <span class="s2">&quot;saddlebrown&quot;</span><span class="p">,</span> <span class="s2">&quot;darkcyan&quot;</span><span class="p">]</span>

<span class="n">estimated_colors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">choose</span><span class="p">(</span><span class="n">estimations</span><span class="p">,</span> <span class="n">colors</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">estimated_colors</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">centroids</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[:</span><span class="n">optimum_k</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">optimum_k</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">centroids_list</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/k_means_3.png"></p>
<p>The algorithm has successfully divided the data into three clusters.</p>
<h1>Observations</h1>
<p>The 2 dimensional application described is useful to provide a simple understanding of the algorithm, of how to find the optimum <em>k</em>, and how to evaluate the performance of the algorithm. However, in real application, it is hardly dealt with 2D data sets where the clusters are clearly visible or detectable using edge detection methods. The goal of clustering algorithms is to find clusters when humans cannot, like, for example, in cases with multidimensional data.</p>
<hr>

<p>The measure to evaluate the performance of the algorithm is the value of the function that is minimized. That function only applies when the data meets the assumptions explained above. However, what happens if the data does not meet those assumptions (it is not convex, the clusters have different sizes, it presents outliers, clusters are superposed), or when it is not possible to know for certain its shape? For those cases, SSE will not be an effective performance measure. For those cases, it would be then preferable to measure the performance of the algorithm by analyzing the points assigned to the clusters created in many replicates. This way, even if the above criteria is not met, either because of unawareness or because it cannot be known (e.g. many dimensions), the measure would give more precise information on the accuracy of the results.</p>
<h1>References</h1>
<p><a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank"><em>k</em>-means clustering - Wikipedia</a></p>
<p><a href="https://realpython.com/k-means-clustering-python/#what-is-clustering" target="_blank">K-Means Clustering in Python: A Practical Guide</a></p>
<p><a href="https://scikit-learn.org/stable/modules/clustering.html#k-means" target="_blank">scikit learn - Clustering - K-means</a></p>
<p><a href="https://towardsdatascience.com/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203" target="_blank">K-means Clustering Python Example</a></p>
<p><a href="http://cs-people.bu.edu/evimaria/Italy-2015/partitional-clustering.pdf" target="_blank">Partitional Clustering</a></p>
<p><a href="https://realpython.com/k-means-clustering-python/#what-is-clustering" target="_blank">K-Means Clustering in Python: A Practical Guide</a></p>
<p><a href="https://blog.learningtree.com/assumptions-ruin-k-means-clusters/" target="_blank">Assumptions Can Ruin Your K-Means Clusters</a></p>
<p><a href="http://varianceexplained.org/r/kmeans-free-lunch/" target="_blank">K-means clustering is not a free lunch</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </section>
      </div>

      <br><br>

    </article>


    <footer>
        <p>
          &copy;
          2021          Gast贸n Amengual
        </p>
    </footer>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-6KC62F9717"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());
  gtag('config', "G-6KC62F9717");</script>  </body>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: { inlineMath: [["$","$"],["\\(","\\)"]] },
    "HTML-CSS": {
      linebreaks: { automatic: true, width: "container" }
    }
    });
  </script>
</html>