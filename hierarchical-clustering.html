<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Meta -->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta charset="utf-8">

    <title>Hierarchical Clustering &ndash; Gastón Amengual</title>

    <!-- PWA -->
    <link rel="manifest" href="https://gastonamengual.github.io/theme/site.webmanifest">
    <meta name="theme-color" content="#6A1A6A">
    <link rel="apple-touch-icon" sizes="180x180" href="https://gastonamengual.github.io/theme/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://gastonamengual.github.io/theme/favicon/favicon-16x16.png">
    <link rel="shortcut icon" type="image/png" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">

    <!-- Social -->
    <meta property="article:author" content="Gastón Amengual" />
    <meta property="article:section" content="Machine Learning" />

    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Hierarchical Clustering"/>
    <meta property="og:description" content="Definition. Steps. From-Scrath Python implementation. Application on data sets of different shapes. Comparison with k-Means."/>
    <meta property="og:site_name" content="Gastón Amengual" />
    <meta property="og:url" content="https://gastonamengual.github.io/hierarchical-clustering.html"/>


    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Spartan:100,200,300,400,500,600,700,800,900">
    <link rel="stylesheet" type="text/css" href="https://gastonamengual.github.io/theme/style.css">
    <link href='fonts/fonts.css' rel='stylesheet'>
  </head>

  <body>
    <div id="navbar">
        <a href="../index.html"><img src="https://gastonamengual.github.io/theme/gaston_amengual.svg" id="navbar-logo"></a>
        <nav id="navbar-menu">
          <ul>
            
              <li class="navbar-li"><a href="/index.html">Home</a></li>
              <li class="navbar-li"><a href="/pages/about.html">About</a></li>
              <li class="navbar-li"><a href="/pages/portfolio.html">Portfolio</a></li>
              <li class="navbar-li"><a href="/categories.html">Articles</a></li>
          </ul>
        </nav>
    </div>



    <br><br>

    <article class="article-article">
      <header class="col-main article-header">
        <h1>Hierarchical Clustering</h1>
        <a class="article-category" href="https://gastonamengual.github.io/category/machine-learning.html">Machine Learning</a>
      </header>

      <div class="col-main">
        <section class="article-content">
          <div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">dendrogram</span><span class="p">,</span> <span class="n">linkage</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Iterable</span>
</code></pre></div>


<h1>Definition</h1>
<p>Hierarchical Clustering is, in data mining and statistics, a method of cluster analysis which seeks to build a hierarchy of clusters. There are two types: </p>
<ul>
<li>Agglomerative clustering </li>
<li>Divisive clustering.</li>
</ul>
<p>A hierarchical cluster consists in a set of nested clusters that are arranged as a tree. Hierarchical methods are especially useful when the target is to arrange the clusters into a natural hierarchy. Unlike other algorithms such as <em>k</em>-means, these methods do not require the number of clusters as a parameter, and one can stop at the desired number of clusters. </p>
<p>In order to decide which clusters should be combined (for agglomerative), or where a cluster should be split (for divisive), a <strong>distance metric</strong> and a <strong>linkage criterion</strong> are used as measures of dissimilarity between sets of observations.</p>
<h2>Distance Metric</h2>
<p>A distance metric must be chosen so that it defines similarity in a way that is sensible for the field of study. There are many distance metrics, e.g. Euclidean distance, Manhattan distance, Maximum distance, Mahalanobis distance. The choice of distance metric should be made based on theoretical concerns from the domain of study. For example, if clustering crime sites in a city, city block distance may be appropriate. Where there is no theoretical justification for an alternative, the Euclidean should generally be preferred, as it is usually the appropriate measure of distance in the physical world.</p>
<h2>Linkage Criteria</h2>
<p>The linkage criteria determines how the distance between two clusters is computed.</p>
<p>Let <span class="math">\(A\)</span> and <span class="math">\(B\)</span> be two clusters, and <span class="math">\(x_A\)</span> and <span class="math">\(x_B\)</span> two point so that <span class="math">\(x_A \in A, x_B \in B\)</span>.</p>
<h3>Single linkage</h3>
<p>It uses the minimum of the distances between all observations of the two clusters.</p>
<p><span class="math">\(d(A, B) = \min \{ d(x_A, x_B) \}\)</span></p>
<h3>Average linkage</h3>
<p>It is the average of the distances of each observation of the two sets.</p>
<p>$d(A, B) = \sum_{i,j} \dfrac{d(x_A, x_B)}{|x_A| \cdot |x_B|} $</p>
<h3>Complete linkage</h3>
<p>It uses the maximum distances between all observations of the two sets.</p>
<p><span class="math">\(d(A, B) = \max \{ d(x_A, x_B) \}\)</span></p>
<h3>Ward's method</h3>
<p>It minimizes the variance of the clusters being merged.</p>
<p>For more information, relate to <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html">Scipy Documentation</a>.</p>
<div class="alert alert-success">
The choice of linkage criteria should be made based on theoretical considerations from the domain of application. A key theoretical issue is the cause of variation. For example, in archaeology, variation is expected to occur through innovation and natural resources, so working out if two groups of artifacts are similar may make sense based on identifying the most similar members of the cluster. Where there are no clear theoretical justifications for the choice of linkage criteria, Ward’s method is the sensible default.
</div>

<h1>Agglomerative Clustering</h1>
<p>Agglomerative Clustering is a type of Hierarchical Clustering in which each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. The main output of Hierarchical Clustering is a <strong>dendrogram</strong>, a tree  diagram that shows the hierarchical relationship between the clusters.</p>
<p>Agglomerative Clustering is useful to find clusters in data where other algorithms (such as <em>k</em>-means) fail, especially when the data is nonspherical or anisotropic, and the potencial clusters have unequal variance or different sizes. Nevertheless, it requires the computation and storage of an <span class="math">\(n \times n\)</span> matrix, which, for very large datasets, can be computationally expensive and slow, and its performance depends strongly on the distance metric and the linkage criterion chosen.</p>
<h2>Steps</h2>
<ol>
<li>Set each observation as a separate cluster.</li>
</ol>
<p><em>Repeat</em></p>
<ol>
<li>
<p>Compute the Cluster Distance Matrix. Each entry <span class="math">\(i,j\)</span> of the matrix is the distance between cluster<span class="math">\(_i\)</span> and cluster<span class="math">\(_j\)</span>, according to a distance metric and linkage criteria.</p>
</li>
<li>
<p>Select clusters <span class="math">\(A\)</span> and <span class="math">\(B\)</span> as the cluster<span class="math">\(_i\)</span> and cluster<span class="math">\(_j\)</span> with minimum distance in the Cluster Distance Matrix.</p>
</li>
<li>
<p>Merge the clusters <span class="math">\(A\)</span> and <span class="math">\(B\)</span>.</p>
</li>
</ol>
<p><em>Until there is one cluster left</em></p>
<h1>Implementation</h1>
<p>Agglomerative Clustering will be implemented using Euclidean Distance and Single Linkage Criterion.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">hierarchical_agglomerative_clustering</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">num_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>

    <span class="c1"># STEP 1: Create one cluster for each point</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">clusters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

    <span class="n">current_num_clusters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusters</span><span class="p">)</span>

    <span class="k">while</span> <span class="n">current_num_clusters</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span> 

        <span class="c1"># STEP 2: Calculate Cluster Distance Matrix</span>

        <span class="c1"># Create empty distance matrix</span>
        <span class="n">cluster_distance_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">current_num_clusters</span><span class="p">,</span> <span class="n">current_num_clusters</span><span class="p">))</span>

        <span class="c1"># Iterate over each element of cluster distance matrix</span>
        <span class="k">for</span> <span class="n">row_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">current_num_clusters</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">column_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">current_num_clusters</span><span class="p">):</span>

                <span class="c1"># If clusters are different</span>
                <span class="k">if</span> <span class="n">row_index</span> <span class="o">!=</span> <span class="n">column_index</span><span class="p">:</span>

                    <span class="c1"># Get points in each cluster</span>
                    <span class="n">points_indexes_cluster_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">flatten</span><span class="p">(</span><span class="n">clusters</span><span class="p">[</span><span class="n">row_index</span><span class="p">]))</span>
                    <span class="n">points_indexes_cluster_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">flatten</span><span class="p">(</span><span class="n">clusters</span><span class="p">[</span><span class="n">column_index</span><span class="p">]))</span>

                    <span class="n">between_clusters_distances</span> <span class="o">=</span> <span class="p">[]</span>

                    <span class="c1"># Iterate over each point index of both clusters</span>
                    <span class="k">for</span> <span class="n">point_index_cluster_1</span> <span class="ow">in</span> <span class="n">points_indexes_cluster_1</span><span class="p">:</span>
                        <span class="k">for</span> <span class="n">point_index_cluster_2</span> <span class="ow">in</span> <span class="n">points_indexes_cluster_2</span><span class="p">:</span>

                            <span class="c1"># Get both points</span>
                            <span class="n">point_cluster_1</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">point_index_cluster_1</span><span class="p">]</span>
                            <span class="n">point_cluster_2</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">point_index_cluster_2</span><span class="p">]</span>

                            <span class="c1"># Calculate Euclidean Distance</span>
                            <span class="n">distance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">point_cluster_1</span> <span class="o">-</span> <span class="n">point_cluster_2</span><span class="p">)</span>

                            <span class="n">between_clusters_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distance</span><span class="p">)</span>

                    <span class="c1"># Set cluster distance matrix entry according to Single Linkage Criterion</span>
                    <span class="n">cluster_distance_matrix</span><span class="p">[</span><span class="n">row_index</span><span class="p">,</span> <span class="n">column_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">between_clusters_distances</span><span class="p">)</span>

        <span class="c1"># Diagonal is 0. Set it to infinity so that it is not chosen as minimum</span>
        <span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">cluster_distance_matrix</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>

        <span class="c1"># STEP 3: Get indexes of the two clusters with minimum distance</span>
        <span class="n">cluster_A</span><span class="p">,</span> <span class="n">cluster_B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">cluster_distance_matrix</span><span class="o">.</span><span class="n">argmin</span><span class="p">(),</span> <span class="n">cluster_distance_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># STEP 4: Merge cluster A and B</span>

        <span class="c1"># Check number of clusters required</span>
        <span class="k">if</span> <span class="n">current_num_clusters</span> <span class="o">==</span> <span class="n">num_clusters</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">clusters</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If cluster A is a list of clusters, append cluster B to the last cluster as a sub list</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">clusters</span><span class="p">[</span><span class="n">cluster_A</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clusters</span><span class="p">[</span><span class="n">cluster_B</span><span class="p">])</span>

            <span class="c1"># If cluster A is not a list of clusters, append cluster B to cluster A as a sub list</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="n">clusters</span><span class="p">[</span><span class="n">cluster_A</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clusters</span><span class="p">[</span><span class="n">cluster_B</span><span class="p">])</span>

            <span class="c1"># Remove cluster B</span>
            <span class="n">clusters</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">cluster_B</span><span class="p">)</span>


        <span class="c1"># Rename dictionary keys</span>
        <span class="n">old_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">clusters</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">new_key</span><span class="p">,</span> <span class="n">old_key</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">old_keys</span><span class="p">):</span>
            <span class="n">clusters</span><span class="p">[</span><span class="n">new_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">clusters</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">old_key</span><span class="p">)</span>

        <span class="c1"># Update number of clusters</span>
        <span class="n">current_num_clusters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusters</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="c1"># Auxiliary function to flatten lists of lists of lists...</span>
<span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">Iterable</span><span class="p">):</span><span class="k">return</span><span class="p">[</span><span class="n">C</span> <span class="k">for</span> <span class="n">B</span> <span class="ow">in</span> <span class="n">x</span> <span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">flatten</span><span class="p">(</span><span class="n">B</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span><span class="k">return</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
</code></pre></div>


<h1>Application on Data Sets of Different Shapes</h1>
<h2>Moons</h2>
<div class="highlight"><pre><span></span><code><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">80</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">noise</span><span class="o">=.</span><span class="mi">05</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">hac</span> <span class="o">=</span> <span class="n">hierarchical_agglomerative_clustering</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">num_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hac</span><span class="p">)):</span>
    <span class="n">cluster</span> <span class="o">=</span> <span class="n">flatten</span><span class="p">(</span><span class="n">hac</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">labels</span><span class="p">[</span><span class="n">cluster</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">choose</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;darkcyan&quot;</span><span class="p">,</span> <span class="s2">&quot;saddlebrown&quot;</span><span class="p">]))</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/hierarchical_clustering_1.png"></p>
<div class="highlight"><pre><span></span><code><span class="n">dendrogram</span><span class="p">(</span><span class="n">linkage</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s1">&#39;single&#39;</span><span class="p">))</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/hierarchical_clustering_2.png"></p>
<p><strong>Comparirson with <span class="math">\(k\)</span>-means</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">choose</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;darkcyan&quot;</span><span class="p">,</span> <span class="s2">&quot;saddlebrown&quot;</span><span class="p">]))</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/hierarchical_clustering_3.png"></p>
<div class="alert alert-warning">
Hierarchical Clustering require a certain amount of observations to perform correctly. Therefore, data sets of different shapes will be used to test the algorithm with Sklearn, as it is optimized to work with large data sets.
</div>

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">AgglomerativeClustering</span>
</code></pre></div>


<h2>Two Spirals</h2>
<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">n_points</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="n">noise</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="mi">780</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="mi">360</span>
<span class="n">d1x</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">n</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
<span class="n">d1y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">n</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">d1x</span><span class="p">,</span> <span class="n">d1y</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="o">-</span><span class="n">d1x</span><span class="p">,</span> <span class="o">-</span><span class="n">d1y</span><span class="p">))))</span>

<span class="n">clustering</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">clustering</span><span class="o">.</span><span class="n">labels_</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">choose</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;darkcyan&quot;</span><span class="p">,</span> <span class="s2">&quot;saddlebrown&quot;</span><span class="p">]))</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/hierarchical_clustering_4.png"></p>
<h2>Contained Circle</h2>
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">factor</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">noise</span><span class="o">=.</span><span class="mi">05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">clustering</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">clustering</span><span class="o">.</span><span class="n">labels_</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">choose</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;darkcyan&quot;</span><span class="p">,</span> <span class="s2">&quot;saddlebrown&quot;</span><span class="p">]))</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/hierarchical_clustering_5.png"></p>
<h2>Different Variance Blobs</h2>
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">clustering</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;ward&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">clustering</span><span class="o">.</span><span class="n">labels_</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">choose</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;darkcyan&quot;</span><span class="p">,</span> <span class="s2">&quot;saddlebrown&quot;</span><span class="p">,</span> <span class="s2">&quot;darkmagenta&quot;</span><span class="p">]))</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/hierarchical_clustering_6.png"></p>
<h1>Hierarchical Divisive Clustering</h1>
<p>The other Hierarchical Clustering algorithm is the Divisive Hierarchical Clustering, which initially groups all the observations into one cluster, and then successively splits these clusters until there is one cluster for each observation. It is rarely used in practice.</p>
<h1>References</h1>
<p><a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" target="_blank">Hierarchical Clustering - Wikipedia</a></p>
<p><a href="https://www.displayr.com/what-is-hierarchical-clustering/#:~:text=Hierarchical%20clustering%2C%20also%20known%20as,broadly%20similar%20to%20each%20other" target="_blank">What is Hierarchical Clustering?
</a></p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" target="_blank">sklearn.cluster.AgglomerativeClustering</a></p>
<p><a href="https://scikit-learn.org/stable/modules/clustering.html" target="_blank">sklearn Clustering</a></p>
<p><a href="https://www.geeksforgeeks.org/difference-between-k-means-and-hierarchical-clustering/" target="_blank">Difference between K means and Hierarchical Clustering</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </section>
      </div>

      <br><br>

    </article>


    <footer>
        <p>
          &copy;
          2021          Gastón Amengual
        </p>
    </footer>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-6KC62F9717"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());
  gtag('config', "G-6KC62F9717");</script>  </body>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: { inlineMath: [["$","$"],["\\(","\\)"]] },
    "HTML-CSS": {
      linebreaks: { automatic: true, width: "container" }
    }
    });
  </script>
</html>