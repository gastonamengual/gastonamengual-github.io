<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Meta -->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta charset="utf-8">

    <title>k-Nearest Neighbors &ndash; Gastón Amengual</title>

    <!-- PWA -->
    <link rel="manifest" href="https://gastonamengual.github.io/theme/site.webmanifest">
    <meta name="theme-color" content="#6A1A6A">
    <link rel="apple-touch-icon" sizes="180x180" href="https://gastonamengual.github.io/theme/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://gastonamengual.github.io/theme/favicon/favicon-16x16.png">
    <link rel="shortcut icon" type="image/png" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">

    <!-- Social -->
    <meta property="article:author" content="Gastón Amengual" />
    <meta property="article:section" content="Machine Learning" />

    <meta property="og:type" content="article"/>
    <meta property="og:title" content="k-Nearest Neighbors"/>
    <meta property="og:description" content="Definition. Assumptions. Finding of optimum k with K-folds Cross-validation. Testing of different distance metrics, including Euclidean, Manhattan and Cosine. Application and analysis on Iris dataset."/>
    <meta property="og:site_name" content="Gastón Amengual" />
    <meta property="og:url" content="https://gastonamengual.github.io/k-nearest-neighbors.html"/>


    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Spartan:100,200,300,400,500,600,700,800,900">
    <link rel="stylesheet" type="text/css" href="https://gastonamengual.github.io/theme/style.css">
    <link href='fonts/fonts.css' rel='stylesheet'>
  </head>

  <body>
    <div id="navbar">
        <a href="../index.html"><img src="https://gastonamengual.github.io/theme/gaston_amengual.svg" id="navbar-logo"></a>
        <nav id="navbar-menu">
          <ul>
            
              <li class="navbar-li"><a href="/index.html">Home</a></li>
              <li class="navbar-li"><a href="/pages/about.html">About</a></li>
              <li class="navbar-li"><a href="/pages/portfolio.html">Portfolio</a></li>
              <li class="navbar-li"><a href="/categories.html">Articles</a></li>
          </ul>
        </nav>
    </div>



    <br><br>

    <article class="article-article">
      <header class="col-main article-header">
        <h1><em>k</em>-Nearest Neighbors</h1>
        <a class="article-category" href="https://gastonamengual.github.io/category/machine-learning.html">Machine Learning</a>
      </header>

      <div class="col-main">
        <section class="article-content">
          <div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="kn">import</span> <span class="n">distance_matrix</span>
<span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="kn">import</span> <span class="n">distance</span>
</code></pre></div>


<h1>1 Definition</h1>
<p><em>k</em>-nearest neighbors algorithm (<em>k</em>-NN) is a classification method developed in 1951. It is a supervised machine learning algorithm used for both classification and regression, although in this work the classification approach will be studied.</p>
<p>The observed data is the input of the algorithm, and it consists of the all the observed data points and their labels. </p>
<h2>1.1 Assumptions</h2>
<ul>
<li>
<p>Similar things exist in close proximity, i.e., similar things are near to each other. The data is not randomly spread.</p>
</li>
<li>
<p>The data is in a feature space (abstract space where each sample is represented as a point in n-dimensional space), in which exists a notion of distance.</p>
</li>
<li>
<p>No assumption on the underlying data distribution is made.</p>
</li>
</ul>
<h2>1.2 Steps</h2>
<ol>
<li>
<p>Calculate the distance from each new point to the observed points.</p>
</li>
<li>
<p>Select the <em>k</em> points with minimum distance.</p>
</li>
<li>
<p>Set the mode (the most frequent) label of the <em>k</em> points as the estimation or prediction for the new point.</p>
</li>
</ol>
<h2>1.3 Considerations</h2>
<ul>
<li>
<p>To improve the algorithm performance, it is recommended to scale the data (between 0 and 1).</p>
</li>
<li>
<p>Although euclidean distance is used the most, other distance metrics such as Canberra, Mahalanobis, Manhattan, Minkowski, Chebyshev, Cosine can be implemented. For more information, read <a href="https://arxiv.org/pdf/1708.04321.pdf">Abu Alfeilat, Haneen Arafat, et al. "Effects of distance measure choice on k-nearest neighbor classifier performance: a review." (2019)</a>.</p>
</li>
<li>
<p>It is considered a <em>lazy</em> algorithm, meaning that it does not learn anything in the training period. The algorithm learns only at the time of making real time predictions.</p>
</li>
<li>
<p>If the new point is considerably far apart from the observed data, <em>k</em>-NN can be used to classify it, but it is probable that the classification is wrong. For preventing such cases, a limit to the distance from a new point to the rest of the data can be set.</p>
</li>
<li>
<p><em>k</em>-NN should not classify points for which two or more labels are equally possible.</p>
</li>
<li>
<p><span class="math">\(k=1\)</span> is often called <em>1-nearest neighbor</em>, and it assigns the new point the label of the nearest point. It might lead to overfitting.</p>
</li>
<li>
<p>In large or high dimensions datasets, it is very expensive to calculate the distances between all points/dimensions.</p>
</li>
<li>
<p>It is sensitive to noisy data, missing values and outliers.</p>
</li>
</ul>
<h1>2 Implementation</h1>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">k_nearest_neighbors</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">test_x</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Train_x can be considered as the training points (cross-validation) or the original data (prediction)</span>
<span class="sd">    Train_y can be considered as the training labels of the orginal points (cross-validation) or the labels of the original data (prediction)</span>
<span class="sd">    Test_x can be considered as the testing points (cross-validation) or the new points (prediction)    </span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c1"># Scale data between 0 and 1 (min-max normalization)</span>
    <span class="n">train_x_min</span> <span class="o">=</span> <span class="n">train_x</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">train_x_max</span> <span class="o">=</span> <span class="n">train_x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">train_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_x</span> <span class="o">-</span> <span class="n">train_x_min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">train_x_max</span> <span class="o">-</span> <span class="n">train_x_min</span><span class="p">)</span>
    <span class="n">test_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">test_x</span> <span class="o">-</span> <span class="n">train_x_min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">train_x_max</span> <span class="o">-</span> <span class="n">train_x_min</span><span class="p">)</span>

    <span class="c1"># Calculate distance from each test point to train points</span>
    <span class="n">distances_matrix</span> <span class="o">=</span> <span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">)</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">distances_matrix</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span> 

    <span class="n">estimations</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">distances</span> <span class="ow">in</span> <span class="n">distances_matrix</span><span class="p">:</span>

        <span class="c1"># Get k train points of minimum distances</span>
        <span class="n">indexes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="n">k</span><span class="p">]</span>

        <span class="c1"># Minimum distance restriction</span>
        <span class="c1"># If the minimum distance is greater than 5% of the distances, no prediction can be done</span>
        <span class="c1"># Threshold should be chosen according to the data domain</span>
        <span class="n">shortest_distance</span> <span class="o">=</span> <span class="n">distances</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">shortest_distance</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="n">estimations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
            <span class="k">continue</span>

        <span class="c1"># Get label corresponding to each train point</span>
        <span class="n">unique</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">train_y</span><span class="p">[</span><span class="n">indexes</span><span class="p">],</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Two-label restriction</span>
        <span class="c1"># When two labels have same frequency, no prediction can be done</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">first</span><span class="p">,</span> <span class="n">second</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">first</span> <span class="o">==</span> <span class="n">second</span><span class="p">:</span>
                <span class="n">estimations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
                <span class="k">continue</span>

        <span class="c1"># Get estimation for test point</span>
        <span class="n">estimation</span> <span class="o">=</span> <span class="n">unique</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">counts</span><span class="p">)]</span>
        <span class="n">estimations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">estimation</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">estimations</span><span class="p">)</span>
</code></pre></div>


<h1>3 Application</h1>
<h2>3.1 Dataset</h2>
<p>The <a href="https://archive.ics.uci.edu/ml/datasets/iris">iris dataset</a> will be used to exemplify the use of the <em>k</em>-NN algorithm. </p>
<div class="highlight"><pre><span></span><code><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mediumpurple&#39;</span><span class="p">,</span> <span class="s1">&#39;lightsalmon&#39;</span><span class="p">,</span> <span class="s1">&#39;lightskyblue&#39;</span><span class="p">,</span> <span class="s1">&#39;darkgrey&#39;</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">choose</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">colors</span><span class="p">))</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/k_nearest_neighbors_1.png"></p>
<h2>3.2 Training Process</h2>
<p>The <em>k</em>-NN algorithm will be trained using K-folds Cross-validation and accuracy (percentage of successes) as the error measure. K-folds will be run for 30 values of <em>k</em>. For each <em>k</em>, the accuracy for each fold will be recorded. The optimum <em>k</em> value will be set as the one which presented the highest accuracy median for every K.</p>
<p>(Note that <em>k</em> refers to <em>k</em>-NN, and K refers to Cross-validation)</p>
<p><br></p>
<p><strong>Train-Validation Split</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">X_train</span><span class="p">,</span> <span class="n">X_validation</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_validation</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div>


<p><strong>K-folds Cross-validation</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">metric</span> <span class="o">=</span> <span class="s1">&#39;euclidean&#39;</span>

<span class="n">folds</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">folds</span><span class="p">)</span>

<span class="n">k_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span>

<span class="n">accuracy_list_training</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_values</span><span class="p">:</span>

    <span class="n">folds_accuracy</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_train</span><span class="p">):</span>

        <span class="n">X_train_fold</span><span class="p">,</span> <span class="n">X_test_fold</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
        <span class="n">y_train_fold</span><span class="p">,</span> <span class="n">y_test_fold</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>

        <span class="n">estimations</span> <span class="o">=</span> <span class="n">k_nearest_neighbors</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">X_train_fold</span><span class="p">,</span> <span class="n">y_train_fold</span><span class="p">,</span> <span class="n">X_test_fold</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">)</span>

        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">estimations</span> <span class="o">==</span> <span class="n">y_test_fold</span><span class="p">)</span>
        <span class="n">folds_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>

    <span class="n">accuracy_list_training</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">folds_accuracy</span><span class="p">)</span>
</code></pre></div>


<h3>3.2.2 Optimum <em>k</em></h3>
<p>Boxplots for the accuracy for each <em>k</em> considering all K folds will be constructed. The optimum <em>k</em> will be the one with maximum median. The median was used instead of the mean to minimize the effects of outliers.</p>
<div class="highlight"><pre><span></span><code><span class="n">accuracy_training_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">accuracy_list_training</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">k_values</span><span class="p">)</span>

<span class="n">best_k_training</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">accuracy_training_df</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">best_accuracy_training</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">accuracy_training_df</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The value of k that presented the best accuracy median in </span><span class="si">{</span><span class="n">folds</span><span class="si">}</span><span class="s1"> folds is: </span><span class="si">{</span><span class="n">best_k_training</span><span class="si">}</span><span class="s1"> with value of </span><span class="si">{</span><span class="n">best_accuracy_training</span><span class="si">:</span><span class="s1">.3g</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">accuracy_training_df</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;maroon&#39;</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="err">The value of k that presented the best accuracy median in 7 folds is: 3 with value of 0.706</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/k_nearest_neighbors_2.png"></p>
<h2>3.3 Test Validation Set</h2>
<p>Once the optimum <em>k</em> was found using Cross-validation, the production environment will be tested using the validation set.</p>
<div class="highlight"><pre><span></span><code><span class="n">estimations</span> <span class="o">=</span> <span class="n">k_nearest_neighbors</span><span class="p">(</span><span class="n">best_k_training</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X_validation</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">)</span>

<span class="n">accuracy_validation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">estimations</span> <span class="o">==</span> <span class="n">y_validation</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Validation Set Accuracy for k=</span><span class="si">{</span><span class="n">best_k_training</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">accuracy_validation</span><span class="si">:</span><span class="s1">.3g</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="err">Validation Set Accuracy for k=3: 0.967</span>
</code></pre></div>


<h1>4 Distance Metrics Comparison</h1>
<p>The training and validation procedure was performed for several distance metrics, and the results are illustrated in the following table, order from best to worst performance in validation set:</p>
<p><br></p>
<div>
<table border="1">
<thead>
  <tr>
    <th class="tg-c3ow">Metric</th>
    <th class="tg-c3ow">Optimum <span style="font-style:italic">*k*</span></th>
    <th class="tg-c3ow">Training</th>
    <th class="tg-c3ow">Validation</th>
  </tr>
</thead>
  <tbody>
    <tr>
    <td class="tg-c3ow">Canberra</td>
    <td class="tg-c3ow">9</td>
    <td class="tg-c3ow">0.706</td>
    <td class="tg-c3ow">0.667</td>
  </tr>
  <tr>
    <td class="tg-c3ow">Mahalanobis</td>
    <td class="tg-c3ow">25</td>
    <td class="tg-c3ow">0.706</td>
    <td class="tg-c3ow">0.667</td>
  </tr>
  <tr>
    <td class="tg-c3ow">Cityblock</td>
    <td class="tg-c3ow">29</td>
    <td class="tg-c3ow">0.765</td>
    <td class="tg-c3ow">0.667</td>
  </tr>
  <tr>
    <td class="tg-c3ow">Euclidean</td>
    <td class="tg-c3ow">3</td>
    <td class="tg-c3ow">0.706</td>
    <td class="tg-c3ow">0.667</td>
  </tr>
  <tr>
    <td class="tg-c3ow">Minkowski</td>
    <td class="tg-c3ow">3</td>
    <td class="tg-c3ow">0.706</td>
    <td class="tg-c3ow">0.667</td>
  </tr>
  <tr>
    <td class="tg-c3ow">Cosine</td>
    <td class="tg-c3ow">17</td>
    <td class="tg-c3ow">0.706</td>
    <td class="tg-c3ow">0.533</td>
  </tr>
  <tr>
    <td class="tg-c3ow">Chebyshev</td>
    <td class="tg-c3ow">13</td>
    <td class="tg-c3ow">0.706</td>
    <td class="tg-c3ow">0.5</td>
  </tr>
  </tbody>
</table>
</div>

<p><br></p>
<p>As can be observed, the five first distances performed similar in both Training and Validation set, with Manhattan performing better in the Training set. The worst distances were Cosine and Chebyshev.</p>
<h1>5 Prediction Map</h1>
<p>To provide an estimate of the classification of different points in the observed data domain, prediction maps for 4 <em>k</em> values (including the optimum) will be made.</p>
<div class="highlight"><pre><span></span><code><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">175</span><span class="p">)</span>

<span class="n">X_space</span><span class="p">,</span> <span class="n">Y_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">X_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">X_space</span><span class="p">)</span>
<span class="n">Y_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">Y_space</span><span class="p">)</span>

<span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">([</span><span class="n">X_space</span><span class="p">,</span> <span class="n">Y_space</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">ks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">17</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">metric</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ks</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;Canberra&#39;</span><span class="p">,</span> <span class="s1">&#39;Cityblock&#39;</span><span class="p">,</span> <span class="s1">&#39;Euclidean&#39;</span><span class="p">,</span> <span class="s1">&#39;Cosine&#39;</span><span class="p">]):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">k_nearest_neighbors</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">points</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">choose</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">colors</span><span class="p">),</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/k_nearest_neighbors_3.png"></p>
<p><br></p>
<p>As can be observed, Manhattan and Euclidean distance provide the best predictions for all points. The cosine distance might be useful when the divisions are not Cartesian but angular and fails to classify the data in the present data set. Moreover, the Manhattan distance has horizontal or vertical lines as boundaries.</p>
<p>Also, it can be seen that adding the minimum distance and tie restriction, although not part of the original algorithm, makes the predictions more real, as the algorithm cannot predict points that are far enough from the rest of the data, as the basic assumption of <em>k</em>-NN is that similar points are close to each other, and a point far apart may belong to a new category not yet observed nor labeled. Furthermore, the two-labels restriction prevents the algorithm from classifying points for which two or more labels are equally possible. It can be noticed that, for all distances metrics, all optimum <em>k</em> values are odd numbers, meaning that these "cannot predict" areas are unlikely to exist between two labeled areas. This gray areas can be seen instead in the intersection between three labeled areas, as depicted in the Canberra and Euclidean cases.</p>
<h1>References</h1>
<p><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" target="_blank">k-nearest neighbors algorithm</a></p>
<p><a href="https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761" target="_blank">Machine Learning Basics with the K-Nearest Neighbors Algorithm
</a></p>
<p><a href="https://stats.stackexchange.com/questions/81240/why-is-knn-not-model-based" target="_blank">Why is KNN not “model-based”?
</a></p>
<p><a href="http://theprofessionalspoint.blogspot.com/2019/02/advantages-and-disadvantages-of-knn.html" target="_blank">Advantages and Disadvantages of KNN</a></p>
<p><a href="https://levelup.gitconnected.com/knn-failure-cases-limitations-and-strategy-to-pick-right-k-45de1b986428" target="_blank">KNN: Failure cases, Limitations</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </section>
      </div>

      <br><br>

    </article>


    <footer>
        <p>
          &copy;
          2021          Gastón Amengual
        </p>
    </footer>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-6KC62F9717"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());
  gtag('config', "G-6KC62F9717");</script>  </body>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: { inlineMath: [["$","$"],["\\(","\\)"]] },
    "HTML-CSS": {
      linebreaks: { automatic: true, width: "container" }
    }
    });
  </script>
</html>