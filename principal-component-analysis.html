<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Meta -->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta charset="utf-8">

    <title>Principal Component Analysis &ndash; Gast贸n Amengual</title>

    <!-- PWA -->
    <link rel="manifest" href="https://gastonamengual.github.io/theme/site.webmanifest">
    <meta name="theme-color" content="#6A1A6A">
    <link rel="apple-touch-icon" sizes="180x180" href="https://gastonamengual.github.io/theme/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://gastonamengual.github.io/theme/favicon/favicon-16x16.png">
    <link rel="shortcut icon" type="image/png" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">

    <!-- Social -->
    <meta property="article:author" content="Gast贸n Amengual" />
    <meta property="article:section" content="Machine Learning" />
    <meta property="article:published_time" content="2020-12-12" />

    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Principal Component Analysis"/>
    <meta property="og:description" content="Theoretical explanation of the SVD, covering three approaches, from-scratch implementation and application on Iris data set."/>
    <meta property="og:site_name" content="Gast贸n Amengual" />
    <meta property="og:url" content="https://gastonamengual.github.io/principal-component-analysis.html"/>


    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Spartan:100,200,300,400,500,600,700,800,900">
    <link rel="stylesheet" type="text/css" href="https://gastonamengual.github.io/theme/css/w3.css">
    <link rel="stylesheet" type="text/css" href="https://gastonamengual.github.io/theme/css/style.css">
    <link href='fonts/fonts.css' rel='stylesheet'>
  </head>

  <body>
    <div class="w3-row w3-card w3-white">
      <header id="header">
        <img src="https://gastonamengual.github.io/theme/gaston_amengual.svg" id="header-logo">
        <nav id="header-menu">
          <ul>
            <li class="header-li"><a href="https://gastonamengual.github.io/">About</a></li>
            <li class="header-li"><a href="https://gastonamengual.github.io/pages/portfolio.html">Portfolio</a></li>
            <li class="header-li"><a href="/categories.html">Articles</a></li>
          </ul>
        </nav>
      </header>
    </div>



    <br><br>

    <article>
      <header class="w3-container col-main" style="display: flex; flex-direction: column;">
        <h1>Principal Component Analysis</h1>
        <span style="margin-top: 1rem;"></span><a style="font-size: 1.6rem;" href="https://gastonamengual.github.io/category/machine-learning.html" title="All articles in category Machine Learning">Machine Learning</a></span>
        <span style="margin-top: 1rem; margin-bottom: 1rem;"><time datetime="2020-12-12T00:00:00+01:00">Sat 12 December 2020</time></span>

         
      </header>



      <div class="col-main w3-container">
        <section id="content">
          <p><strong>Notebook written by Gast贸n Amengual.</strong></p>
<hr>

<p><strong>Principal Component Analysis (PCA)</strong> is a dimensionality reduction method that transforms a large set of variables into a smaller one while keeping most of the variance present in the original set. </p>
<p>PCA is defined as a linear transformation that transforms the data into a new orthogonal coordinate system. The new coordinates are built such that the greatest variance lies on the first coordinate or principal component, the second greatest variance on the second coordinate, and so on. This transformation is found by maximizing the variance over the projected points.</p>
<p>Let <span class="math">\(X\)</span> be an <span class="math">\(n \times m\)</span> matrix where each row represents a different observation or measurement of the experiment, and each column a dimension or feature. However, it is also assumed that the observations have mean <span class="math">\(0\)</span> (i.e., <span class="math">\(X\)</span> has column-wise zero empirical mean)</p>
<p>PCA finds a new set of orthogonal coordinates (i.e. linearly independent, and non-correlated) and ranks each according to the explained variance of the data along them. </p>
<p>The new coordinates found are called <strong>principal components</strong>, which are new features constructed as linear combinations of the initial features. This allows to reduce dimensionality while keeping underlying information structure. Since the principal components are ranked, it is possible to discard those with low contribution (i.e. the added explained variance is negligible). It is important to note that these new features are harder to interpret than the original ones, as they are a linear combinations that does not necessarily have an intrinsic meaning.</p>
<p>Each principal component is built as a linear combination of the original features and scaled according to a scalar factor. A <strong>scree plot</strong> is a line plot of these factors, it is used to show the contribution of each principal component to the explained variance. As a consequence, as the components are ranked, the scree plot always displays the factors in descending order by magnitude.</p>
<h2>Geometric approach</h2>
<p>A zero-intercept line is to be found such that the variance of the projected data points over the line is maximized. The principal components represent the directions of the data (projection) that explains the maximal amount of variance, and can be thought as new axes providing the best angle to capture the data dispersion.</p>
<p>Considering a data point, and a line drawn through the origin, then:</p>
<ul>
<li>Let <span class="math">\(a\)</span> be the position vector of the point. </li>
<li>Let <span class="math">\(b\)</span> be the perpendicular distance vector from the point to the line.</li>
<li>Let <span class="math">\(c\)</span> be the position vector of the projected point.</li>
</ul>
<p>All three vectors form a square triangle such that, according to the Pythagoras theorem, <span class="math">\(a^2 = b^2 + c^2\)</span>. <span class="math">\(b\)</span> and <span class="math">\(c\)</span> are inversely related and, since <span class="math">\(a\)</span> is fixed, if <span class="math">\(b\)</span> gets bigger, <span class="math">\(c\)</span> gets smaller, and vice versa. As the objective is to maximize variance (dispersion), PCA can either minimize the distance to the line (<span class="math">\(b\)</span>) or maximize the distance from the projected point to the origin (<span class="math">\(c\)</span>).</p>
<p><img src="https://liorpachter.files.wordpress.com/2014/05/pca_figure1.jpg" width=500 /></p>
<h2>Covariance approach</h2>
<p>As an alternative, the principal components can be also thought as eigenvectors of the data covariance matrix, and can be computed by eigendecomposition of the data covariance matrix. </p>
<p>Let <span class="math">\(X\)</span> be an <span class="math">\(n \times m\)</span> matrix where each row represents a different observation or measurement of the experiment, and each column a dimension or feature. </p>
<p>Let <span class="math">\(B\)</span> be an <span class="math">\(n \times m\)</span> matrix where each column is computed by subtracting the column-wise mean <span class="math">\(\bar{X}\)</span> from each column of <span class="math">\(X\)</span>, namely, <span class="math">\(B = X - \bar{X}\)</span>.</p>
<ol>
<li>
<p><strong>Calculate the covariance matrix <span class="math">\(C\)</span></strong>, such that <span class="math">\(C = \dfrac{B^TB}{n-1}\)</span>.</p>
</li>
<li>
<p><strong>Compute the eigenvectors (<span class="math">\(V\)</span>) and eigenvalues (<span class="math">\(\Lambda\)</span>) of <span class="math">\(C\)</span></strong>, such that <span class="math">\(CV = \Lambda V\)</span>.</p>
</li>
<li>
<p><strong>Sort the eigenvectors according to their eigenvalues in decreasing order</strong>.</p>
</li>
<li>
<p><strong>Choose the first <span class="math">\(r\)</span> eigenvectors that will be the new <span class="math">\(r\)</span> dimensions</strong>. Select a subset <span class="math">\(W\)</span> of the eigenvectors as basis vectors.</p>
</li>
<li>
<p><strong>Compute the projected data matrix <span class="math">\(T\)</span></strong>, such that <span class="math">\(T = B W\)</span>.</p>
</li>
</ol>
<h2>SVD approach</h2>
<p>Another approach based on eigendecomposition is calculating PCA as a subset of the Singular Value Decomposition (SVD).</p>
<p>Let <span class="math">\(X\)</span> be an <span class="math">\(n \times m\)</span> matrix where each row represents a different observation or measurement of the experiment, and each column a dimension or feature. </p>
<p>Let <span class="math">\(B\)</span> be an <span class="math">\(n \times m\)</span> matrix where each column is computed by subtracting the column-wise mean <span class="math">\(\bar{X}\)</span> from each column of <span class="math">\(X\)</span>, namely, <span class="math">\(B = X - \bar{X}\)</span>.</p>
<ol>
<li>
<p><strong>Compute the SVD</strong>. <span class="math">\(B = U \Sigma V^T\)</span> </p>
</li>
<li>
<p><strong>Choose the first <span class="math">\(r\)</span> rows from <span class="math">\(U\)</span> and first <span class="math">\(r\)</span> singular (new <span class="math">\(r\)</span> dimensions)</strong>.</p>
</li>
<li>
<p><strong>Compute the projected data matrix <span class="math">\(T\)</span></strong>, such that <span class="math">\(T = XV = U \Sigma V^T V = U \Sigma\)</span>. This form is also the polar decomposition of <span class="math">\(T\)</span>.</p>
</li>
</ol>
<h2>Relation between Covariance and SVD approaches</h2>
<p>According to the SVD, </p>
<div class="math">$$B = U \Sigma V^T$$</div>
<div class="math">$$B^TB = V \Sigma U^T U \Sigma V^T = V \Sigma^2 V^T$$</div>
<p>The covariance matrix of <span class="math">\(B\)</span> is <span class="math">\(C = \dfrac{B^TB}{n-1} = \dfrac{V \Sigma^2 V^T}{n-1}\)</span>.</p>
<p><span class="math">\(\Sigma\)</span> are the eigenvalues of the matrix <span class="math">\(B\)</span>, while <span class="math">\(\Lambda\)</span> are the eigenvalues of <span class="math">\(C\)</span>. The relationship between them is defined as <span class="math">\(\Lambda = \dfrac{\Sigma^2}{n-1}\)</span>.</p>
<p>Moreover, the right singular vectors <span class="math">\(V\)</span> of <span class="math">\(B\)</span> are equivalent to the eigenvectors of <span class="math">\(B^TB\)</span>.</p>
<h2>Implementation</h2>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">scree_variance_explained_plot</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="n">explained_variance_ratio</span><span class="p">):</span>
    <span class="c1"># Scree plot</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkcyan&#39;</span><span class="p">)</span>

    <span class="c1"># Variance explained plot</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">explained_variance_ratio</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;saddlebrown&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">project_1D</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">method</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;saddlebrown&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">project_2D</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">method</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">130</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">principal_component_analysis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">approach</span><span class="o">=</span><span class="s1">&#39;svd&#39;</span><span class="p">):</span>

    <span class="c1"># Center data</span>
    <span class="n">X</span> <span class="o">-=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">approach</span> <span class="o">==</span> <span class="s1">&#39;covariance&#39;</span><span class="p">:</span>

        <span class="c1"># Covariance matrix</span>
        <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>

        <span class="c1"># Eigendecomposition</span>
        <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">covariance_matrix</span><span class="p">)</span>
        <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="o">.</span><span class="n">T</span>

        <span class="c1"># Sort eigenvectors</span>
        <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span>
        <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span>

        <span class="c1"># Keep n components</span>
        <span class="k">if</span> <span class="n">n_components</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_components</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">eigenvectors</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n_components</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_components</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="o">.</span><span class="n">size</span>

        <span class="n">components</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span>

        <span class="c1"># Transform data</span>
        <span class="n">transformed_data</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">components</span><span class="o">.</span><span class="n">T</span>

        <span class="c1"># Attributes </span>
        <span class="n">explained_variance</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span>
        <span class="n">explained_variance_ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">explained_variance</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">))])</span>

        <span class="n">singular_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">explained_variance</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> 

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;components&#39;</span><span class="p">:</span> <span class="n">components</span><span class="p">,</span> 
                <span class="s1">&#39;explained_variance&#39;</span><span class="p">:</span> <span class="n">explained_variance</span><span class="p">,</span> 
                <span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">:</span> <span class="n">explained_variance_ratio</span><span class="p">,</span> 
                <span class="s1">&#39;singular_values&#39;</span><span class="p">:</span> <span class="n">singular_values</span><span class="p">,</span> 
                <span class="s1">&#39;transformed_data&#39;</span><span class="p">:</span> <span class="n">transformed_data</span><span class="p">,}</span>

    <span class="k">if</span> <span class="n">approach</span> <span class="o">==</span> <span class="s1">&#39;svd&#39;</span><span class="p">:</span>

        <span class="c1"># Compute SVD</span>
        <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Keep n components</span>
        <span class="k">if</span> <span class="n">n_components</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_components</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">S</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n_components</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_components</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">size</span>

        <span class="c1"># Transform data</span>
        <span class="n">transformed_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_components</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)[:</span><span class="n">n_components</span><span class="p">])[:,</span> <span class="p">:</span><span class="n">n_components</span><span class="p">]</span>

        <span class="c1"># Attributes</span>
        <span class="n">components</span> <span class="o">=</span> <span class="n">VT</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_components</span><span class="p">]</span>

        <span class="n">explained_variance</span> <span class="o">=</span> <span class="n">S</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">explained_variance_ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">explained_variance</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">))])</span>

        <span class="n">singular_values</span> <span class="o">=</span> <span class="n">S</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;components&#39;</span><span class="p">:</span> <span class="n">components</span><span class="p">,</span> 
                <span class="s1">&#39;explained_variance&#39;</span><span class="p">:</span> <span class="n">explained_variance</span><span class="p">,</span> 
                <span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">:</span> <span class="n">explained_variance_ratio</span><span class="p">,</span> 
                <span class="s1">&#39;singular_values&#39;</span><span class="p">:</span> <span class="n">singular_values</span><span class="p">,</span> 
                <span class="s1">&#39;transformed_data&#39;</span><span class="p">:</span> <span class="n">transformed_data</span><span class="p">,}</span>

    <span class="k">if</span> <span class="n">approach</span> <span class="o">==</span> <span class="s1">&#39;sklearn&#39;</span><span class="p">:</span>        
        <span class="n">pca</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">)</span>
        <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;components&#39;</span><span class="p">:</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> 
                <span class="s1">&#39;explained_variance&#39;</span><span class="p">:</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">,</span> 
                <span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">:</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">,</span> 
                <span class="s1">&#39;singular_values&#39;</span><span class="p">:</span> <span class="n">pca</span><span class="o">.</span><span class="n">singular_values_</span><span class="p">,</span> 
                <span class="s1">&#39;transformed_data&#39;</span><span class="p">:</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">),}</span>
</code></pre></div>


<h2>Application</h2>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">sklearn.decomposition</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">iris_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv&#39;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris_df</span><span class="p">[[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal_width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_width&#39;</span><span class="p">]]</span>
<span class="n">num_components</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">approaches</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;covariance&#39;</span><span class="p">,</span> <span class="s1">&#39;svd&#39;</span><span class="p">,</span> <span class="s1">&#39;sklearn&#39;</span><span class="p">]</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="c1"># TRANSFORMED DATA</span>


<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">approach</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">approaches</span><span class="p">):</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">principal_component_analysis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">num_components</span><span class="p">,</span> <span class="n">approach</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pca</span><span class="p">[</span><span class="s1">&#39;transformed_data&#39;</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">pca</span><span class="p">[</span><span class="s1">&#39;transformed_data&#39;</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="c1"># EXPLAINED VARIANCE RATIO</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">approach</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">approaches</span><span class="p">):</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">principal_component_analysis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">num_components</span><span class="p">,</span> <span class="n">approach</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">pca</span><span class="p">[</span><span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;saddlebrown&#39;</span><span class="p">)</span>

<span class="c1"># CUMULATIVE VARIANCE RATIO</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">approach</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">approaches</span><span class="p">):</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">principal_component_analysis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">num_components</span><span class="p">,</span> <span class="n">approach</span><span class="p">)</span>
    <span class="n">cum_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="p">[</span><span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">cum_variance</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;olivedrab&#39;</span><span class="p">)</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/principal_component_analysis_1.png"></p>
<p><img alt="image alt text" src="https://gastonamengual.github.io/images/principal_component_analysis_2.png"></p>
<p><img alt="image alt text" src="https://gastonamengual.github.io/images/principal_component_analysis_3.png"></p>
<hr>

<h1>References</h1>
<p>https://builtin.com/data-science/step-step-explanation-principal-component-analysis</p>
<p>https://towardsdatascience.com/principal-component-analysis-pca-from-scratch-in-python-7f3e2a540c51</p>
<p>https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/</p>
<p>https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/decomposition/pca.py#L385</p>
<p>Eigenvectors and eigenvalues | Essence of linear algebra, chapter 14 - 3Blue1Brown - https://www.youtube.com/watch?v=PFDu9oVAE-g</p>
<p>PCA (Principal Component Analysis) in Python -  Python Engineer - https://www.youtube.com/watch?v=52d7ha-GdV8&amp;t=3s</p>
<p>Principal Component Analysis (PCA), Step-by-Step - StatQuest - https://www.youtube.com/watch?v=FgakZw6K1QQ&amp;feature=emb_title</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </section>

        <br><br>

        <footer>


        </footer>
      </div>
    </article>


    <footer id="footer">
      <div id="footer-copyright" class="w3-center w3-large w3-text-white w3-padding-48">
        <span>
          &copy;
          2021          Gast贸n Amengual
        </span>
      </div>
    </footer>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-6KC62F9717"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());
  gtag('config', "G-6KC62F9717");</script>    </body>
</html>