<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Meta -->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta charset="utf-8">

    <title>Principal Component Analysis &ndash; Gastón Amengual</title>

    <!-- PWA -->
    <link rel="manifest" href="https://gastonamengual.github.io/theme/site.webmanifest">
    <meta name="theme-color" content="#6A1A6A">
    <link rel="apple-touch-icon" sizes="180x180" href="https://gastonamengual.github.io/theme/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://gastonamengual.github.io/theme/favicon/favicon-16x16.png">
    <link rel="shortcut icon" type="image/png" href="https://gastonamengual.github.io/theme/favicon/favicon-32x32.png">

    <!-- Social -->
    <meta property="article:author" content="Gastón Amengual" />
    <meta property="article:section" content="Machine Learning" />

    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Principal Component Analysis"/>
    <meta property="og:description" content="Theoretical explanation of PCA, including Geometric, Covariance, and SVD approaches. From-scratch Python implementation. Application to real dataset. Why PCA should not be used for clustering."/>
    <meta property="og:site_name" content="Gastón Amengual" />
    <meta property="og:url" content="https://gastonamengual.github.io/principal-component-analysis.html"/>


    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Spartan:100,200,300,400,500,600,700,800,900">
    <link rel="stylesheet" type="text/css" href="https://gastonamengual.github.io/theme/style.css">
    <link href='fonts/fonts.css' rel='stylesheet'>
  </head>

  <body>
    <div id="navbar">
        <a href="../index.html"><img src="https://gastonamengual.github.io/theme/gaston_amengual.svg" id="navbar-logo"></a>
        <nav id="navbar-menu">
          <ul>
            
              <li class="navbar-li"><a href="/index.html">Home</a></li>
              <li class="navbar-li"><a href="/pages/about.html">About</a></li>
              <li class="navbar-li"><a href="/pages/portfolio.html">Portfolio</a></li>
              <li class="navbar-li"><a href="/categories.html">Articles</a></li>
          </ul>
        </nav>
    </div>



    <br><br>

    <article class="article-article">
      <header class="col-main article-header">
        <h1>Principal Component Analysis</h1>
        <a class="article-category" href="https://gastonamengual.github.io/category/machine-learning.html">Machine Learning</a>
      </header>

      <div class="col-main">
        <section class="article-content">
          <p><meta property="og:image" content="{static}../images/principal_component_analysis_2.png"/></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
</code></pre></div>


<h1>1 Definition</h1>
<p><strong>Principal Component Analysis (PCA)</strong> is a dimensionality reduction method that transforms the data into a new orthogonal (i.e. perpendicular, linearly independent, and non-correlated) coordinate system that <strong>keeps most of the variance</strong> that was present in the original data. </p>
<p>PCA transforms the data <strong>maximizing the variance</strong> over the projected points. Each new coordinate is called <strong>principal component</strong>, and is ranked according to how much variance they explain, or by <strong>explained variance</strong> (the greatest variance lies on the first principal component, the second greatest variance on the second principal component, ...).</p>
<p>The data is arranged in a an <span class="math">\(n \times m\)</span> matrix <span class="math">\(X\)</span>, where each row represents a different observation or measurement of the experiment, and each column represents a dimension or feature. </p>
<p>The principal components are new features constructed as linear combinations of the initial features, allowing to reduce dimensionality while keeping underlying information structure. Since the principal components are ranked, it is possible to discard those with low contribution (i.e. the added explained variance is negligible). However, these new features are harder to interpret than the original ones, as they are a linear combinations that does not necessarily have an intrinsic meaning.</p>
<h2>1.1 Assumptions</h2>
<p>In order to apply PCA, the following assumptions must be met.</p>
<ol>
<li>
<p>PCA assumes that the observations have mean <span class="math">\(0\)</span> (i.e., <span class="math">\(X\)</span> has column-wise zero empirical mean).</p>
</li>
<li>
<p>The data must be able to be analyzed with a covariance matrix, which in term makes sense when the relation between the data is linear.</p>
</li>
<li>
<p>The sample taken is large enough and there are no significant outliers (heavy-tailed distributions should be avoided).</p>
</li>
</ol>
<h1>2 Approaches</h1>
<h2>2.1 Geometric approach</h2>
<p>A zero-intercept line is to be found such that the variance of the projected data points over the line is maximized. The principal components represent the directions of the data (projection) that explains the maximal amount of variance, and can be thought as new axes providing the best angle to capture the data dispersion.</p>
<p>Considering a data point, and a line drawn through the origin, then:</p>
<ul>
<li>Let <span class="math">\(a\)</span> be the position vector of the point. </li>
<li>Let <span class="math">\(b\)</span> be the perpendicular distance vector from the point to the line.</li>
<li>Let <span class="math">\(c\)</span> be the position vector of the projected point.</li>
</ul>
<p><img src="https://liorpachter.files.wordpress.com/2014/05/pca_figure1.jpg" style="width: 350px;" /></p>
<p>All three vectors form a square triangle such that, according to the Pythagoras theorem, <span class="math">\(a^2 = b^2 + c^2\)</span>. <span class="math">\(b\)</span> and <span class="math">\(c\)</span> are inversely related and, since <span class="math">\(a\)</span> is fixed, if <span class="math">\(b\)</span> gets bigger, <span class="math">\(c\)</span> gets smaller, and vice versa. As the objective is to maximize variance (dispersion), PCA can either minimize the distance to the line (<span class="math">\(b\)</span>) or maximize the distance from the projected point to the origin (<span class="math">\(c\)</span>).</p>
<p>For more information, relate to <a href="https://www.youtube.com/watch?v=FgakZw6K1QQ&amp;feature=emb_title" target="_blank">Principal Component Analysis (PCA), Step-by-Step - StatQuest</a>.</p>
<h2>2.2 Covariance approach</h2>
<p>As an alternative, the principal components can be also thought as eigenvectors of the data covariance matrix, and can be computed by eigendecomposition of the data covariance matrix. </p>
<p>Let <span class="math">\(X\)</span> be an <span class="math">\(n \times m\)</span> matrix where each row represents a different observation or measurement of the experiment, and each column a dimension or feature. </p>
<ol>
<li>
<p><strong>Calculate <span class="math">\(B\)</span>, an <span class="math">\(n \times m\)</span> matrix where each column is computed by subtracting the column-wise mean <span class="math">\(\bar{X}\)</span> from each column of <span class="math">\(X\)</span>, namely, <span class="math">\(B = X - \bar{X}\)</span>.</strong></p>
</li>
<li>
<p><strong>Calculate the covariance matrix <span class="math">\(C\)</span></strong>, such that <span class="math">\(C = \dfrac{B^TB}{n-1}\)</span>.</p>
</li>
<li>
<p><strong>Compute the eigenvectors (<span class="math">\(V\)</span>) and eigenvalues (<span class="math">\(\Lambda\)</span>) of <span class="math">\(C\)</span></strong>, such that <span class="math">\(CV = \Lambda V\)</span>.</p>
</li>
<li>
<p><strong>Sort the eigenvectors according to their eigenvalues in decreasing order</strong>.</p>
</li>
<li>
<p><strong>Choose the first <span class="math">\(r\)</span> eigenvectors that will be the new <span class="math">\(r\)</span> dimensions</strong>. Select a subset <span class="math">\(W\)</span> of the eigenvectors as basis vectors.</p>
</li>
<li>
<p><strong>Compute the transformed data matrix <span class="math">\(T\)</span></strong>, such that <span class="math">\(T = B W\)</span>.</p>
</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">principal_component_analysis_covariance</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>

    <span class="c1"># The function takes a Pandas DataFrame</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># 1 Calculate B: Center data</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">B</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

    <span class="c1"># 2 Calculate C: Covariance matrix</span>
    <span class="n">C</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">B</span><span class="p">)</span>

    <span class="c1"># 3 Eigendecomposition</span>
    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
    <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="o">.</span><span class="n">T</span>

    <span class="c1"># 4 Sort eigenvectors and eigenvalues</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span>
    <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span>

    <span class="c1"># The 5th step will be performed outside this function. The full transformed data matrix </span>
    <span class="c1"># will be returned, and the r principal components will be chosen later. If desired, this</span>
    <span class="c1"># function could take the number of components as a parameter, and the subset of the </span>
    <span class="c1"># eigenvectors can be selected here. </span>

    <span class="c1"># 6 Compute transformed data matrix and atributes</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">eigenvectors</span>
    <span class="n">explained_variance</span> <span class="o">=</span> <span class="n">eigenvalues</span>
    <span class="n">explained_variance_ratio</span> <span class="o">=</span> <span class="n">explained_variance</span> <span class="o">/</span> <span class="n">explained_variance</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">singular_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">explained_variance</span> <span class="o">*</span> <span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># Explained in section 4</span>
    <span class="n">transformed_data</span> <span class="o">=</span> <span class="n">B</span> <span class="o">@</span> <span class="n">components</span><span class="o">.</span><span class="n">T</span>

    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;components&#39;</span><span class="p">:</span> <span class="n">components</span><span class="p">,</span> 
            <span class="s1">&#39;explained_variance&#39;</span><span class="p">:</span> <span class="n">explained_variance</span><span class="p">,</span> 
            <span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">:</span> <span class="n">explained_variance_ratio</span><span class="p">,</span> 
            <span class="s1">&#39;singular_values&#39;</span><span class="p">:</span> <span class="n">singular_values</span><span class="p">,</span> 
            <span class="s1">&#39;transformed_data&#39;</span><span class="p">:</span> <span class="n">transformed_data</span><span class="p">,}</span>
</code></pre></div>


<h2>2.3 SVD approach</h2>
<p>Another approach based on eigendecomposition is calculating PCA as a subset of the Singular Value Decomposition (SVD).</p>
<p>Let <span class="math">\(X\)</span> be an <span class="math">\(n \times m\)</span> matrix where each row represents a different observation or measurement of the experiment, and each column a dimension or feature. </p>
<ol>
<li>
<p><strong>Calculate <span class="math">\(B\)</span>, an <span class="math">\(n \times m\)</span> matrix where each column is computed by subtracting the column-wise mean <span class="math">\(\bar{X}\)</span> from each column of <span class="math">\(X\)</span>, namely, <span class="math">\(B = X - \bar{X}\)</span>.</strong></p>
</li>
<li>
<p><strong>Compute the SVD</strong>. <span class="math">\(B = U \Sigma V^T\)</span> </p>
</li>
<li>
<p><strong>Choose the first <span class="math">\(r\)</span> rows from <span class="math">\(U\)</span> and first <span class="math">\(r\)</span> singular (new <span class="math">\(r\)</span> dimensions)</strong>.</p>
</li>
<li>
<p><strong>Compute the transformed data matrix <span class="math">\(T\)</span></strong>, such that <span class="math">\(T = XV = U \Sigma V^T V = U \Sigma\)</span>. This form is also the polar decomposition of <span class="math">\(T\)</span>.</p>
</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">principal_component_analysis_svd</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>

    <span class="c1"># The function takes a Pandas DataFrame</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># 1 Calculate B: Center data</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">B</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

    <span class="c1"># 2 Compute SVD</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># The 3rd step will be performed outside this function. The full transformed data </span>
    <span class="c1"># matrix will be returned, and the r principal components will be chosen later. If</span>
    <span class="c1"># desired, this function could take the number of components as a parameter, and </span>
    <span class="c1"># the subset of the eigenvectors can be selected here. </span>

    <span class="c1"># 4 Compute transformed data matrix and atributes</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">VT</span>
    <span class="n">explained_variance</span> <span class="o">=</span> <span class="n">S</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Explained in section 4</span>
    <span class="n">explained_variance_ratio</span> <span class="o">=</span> <span class="n">explained_variance</span> <span class="o">/</span> <span class="n">explained_variance</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">singular_values</span> <span class="o">=</span> <span class="n">S</span>
    <span class="n">transformed_data</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;components&#39;</span><span class="p">:</span> <span class="n">components</span><span class="p">,</span> 
            <span class="s1">&#39;explained_variance&#39;</span><span class="p">:</span> <span class="n">explained_variance</span><span class="p">,</span> 
            <span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">:</span> <span class="n">explained_variance_ratio</span><span class="p">,</span> 
            <span class="s1">&#39;singular_values&#39;</span><span class="p">:</span> <span class="n">singular_values</span><span class="p">,</span> 
            <span class="s1">&#39;transformed_data&#39;</span><span class="p">:</span> <span class="n">transformed_data</span><span class="p">,}</span>
</code></pre></div>


<h2>2.4 Relation between Covariance and SVD approaches</h2>
<p>According to the SVD, </p>
<div class="math">$$B = U \Sigma V^T$$</div>
<div class="math">$$B^TB = V \Sigma U^T U \Sigma V^T = V \Sigma^2 V^T$$</div>
<p>The covariance matrix of <span class="math">\(B\)</span> is <span class="math">\(C = \dfrac{B^TB}{n-1} = \dfrac{V \Sigma^2 V^T}{n-1}\)</span>.</p>
<p><span class="math">\(\Sigma\)</span> are the eigenvalues of the matrix <span class="math">\(B\)</span>, while <span class="math">\(\Lambda\)</span> are the eigenvalues of <span class="math">\(C\)</span>. The relationship between them is defined as <span class="math">\(\Lambda = \dfrac{\Sigma^2}{n-1}\)</span>.</p>
<p>Moreover, the right singular vectors <span class="math">\(V\)</span> of <span class="math">\(B\)</span> are equivalent to the eigenvectors of <span class="math">\(B^TB\)</span>.</p>
<hr>

<h1>Application</h1>
<h3>Ozone Dataset</h3>
<p>Forecasting skewed biased stochastic ozone days: analyses, solutions and beyond, Knowledge and Information Systems, Vol. 14, No. 3, 2008.</p>
<p>The dataset contains 2534 observations and 73 features. <strong>Author</strong>: Kun Zhang, Wei Fan, XiaoJing Yuan. <strong>Source</strong>: <a href="https://archive.ics.uci.edu/ml/datasets/ozone+level+detection" target="_blank">UCI</a>.</p>
<hr>

<div class="highlight"><pre><span></span><code><span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;ozone-level-8hr.csv&#39;</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>


<div class="table-div">
<table border="1">
  <thead>
    <tr>
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>...</th>
      <th>V64</th>
      <th>V65</th>
      <th>V66</th>
      <th>V67</th>
      <th>V68</th>
      <th>V69</th>
      <th>V70</th>
      <th>V71</th>
      <th>V72</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.8</td>
      <td>1.8</td>
      <td>2.4</td>
      <td>2.1</td>
      <td>2.0</td>
      <td>2.1</td>
      <td>1.5</td>
      <td>1.7</td>
      <td>1.9</td>
      <td>2.3</td>
      <td>...</td>
      <td>0.15</td>
      <td>10.67</td>
      <td>-1.56</td>
      <td>5795</td>
      <td>-12.1</td>
      <td>17.9</td>
      <td>1033</td>
      <td>-55</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.8</td>
      <td>3.2</td>
      <td>3.3</td>
      <td>2.7</td>
      <td>3.3</td>
      <td>3.2</td>
      <td>2.9</td>
      <td>2.8</td>
      <td>3.1</td>
      <td>3.4</td>
      <td>...</td>
      <td>0.48</td>
      <td>8.39</td>
      <td>3.84</td>
      <td>5805</td>
      <td>14.05</td>
      <td>29</td>
      <td>10275</td>
      <td>-55</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.9</td>
      <td>2.8</td>
      <td>2.6</td>
      <td>2.1</td>
      <td>2.2</td>
      <td>2.5</td>
      <td>2.5</td>
      <td>2.7</td>
      <td>2.2</td>
      <td>2.5</td>
      <td>...</td>
      <td>0.6</td>
      <td>6.94</td>
      <td>9.8</td>
      <td>5790</td>
      <td>17.9</td>
      <td>41.3</td>
      <td>10235</td>
      <td>-40</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.7</td>
      <td>3.8</td>
      <td>3.7</td>
      <td>3.8</td>
      <td>2.9</td>
      <td>3.1</td>
      <td>2.8</td>
      <td>2.5</td>
      <td>2.4</td>
      <td>3.1</td>
      <td>...</td>
      <td>0.49</td>
      <td>8.73</td>
      <td>10.54</td>
      <td>5775</td>
      <td>31.15</td>
      <td>51.7</td>
      <td>10195</td>
      <td>-40</td>
      <td>2.08</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.6</td>
      <td>2.1</td>
      <td>1.6</td>
      <td>1.4</td>
      <td>0.9</td>
      <td>1.5</td>
      <td>1.2</td>
      <td>1.4</td>
      <td>1.3</td>
      <td>1.4</td>
      <td>...</td>
      <td>0.30</td>
      <td>9.87</td>
      <td>0.83</td>
      <td>5818</td>
      <td>10.51</td>
      <td>37.38</td>
      <td>10164</td>
      <td>-0.119</td>
      <td>0.58</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p><br>
<p>2534 rows × 73 columns</p></p>
<p><img alt="image alt text" src="https://gastonamengual.github.io/images/principal_component_analysis_1.png"></p>
<p>(For more information of the correlogram implementation, please visit <a href="https://gist.github.com/ELC/d2a0c4fdd05fdf61218f35ccc248479d" target="_blank">this site</a>)</p>
<p><br></p>
<p><strong>Run PCA</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">pca</span> <span class="o">=</span> <span class="n">principal_component_analysis_covariance</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">x_ax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pca</span><span class="p">[</span><span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_ax</span><span class="p">,</span> <span class="n">pca</span><span class="p">[</span><span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>

<span class="n">xticks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pca</span><span class="p">[</span><span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">xticks</span><span class="p">)</span>
</code></pre></div>


<p><img alt="image alt text" src="https://gastonamengual.github.io/images/principal_component_analysis_2.png"></p>
<p><br></p>
<p>The plot above is a <strong>scree plot</strong>, and it shows the contribution of each principal component to the explained variance (ratio). It displays the principal components in descending order of eigenvalues or explained variance. It is useful to analyze which principal components capture most of the variance of the original data. In this case, it can be observed that beyond 5 principal components, no significant amount of data is explained.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Step 5 performed after transforming data.</span>
<span class="n">n_components</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">explained_variance_n_components</span> <span class="o">=</span> <span class="n">explained_variance_ratio</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span>
<span class="n">transformed_data_n_components</span> <span class="o">=</span> <span class="n">pca</span><span class="p">[</span><span class="s1">&#39;transformed_data&#39;</span><span class="p">][:,:</span><span class="n">n_components</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The first </span><span class="si">{</span><span class="n">n_components</span><span class="si">}</span><span class="s1"> components explain </span><span class="si">{</span><span class="n">explained_variance_n_components</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s1">.3g</span><span class="si">}</span><span class="s1">% of the original dataset variance&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">n_components</span> <span class="o">/</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.3g</span><span class="si">}</span><span class="s1">% of the original number of columns are kept&#39;</span><span class="p">)</span>

<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;PC</span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">)]</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">transformed_data_n_components</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="err">The first 5 components explain 0.976% of the original dataset variance</span>
<span class="err">0.0685% of the original number of columns are kept</span>
</code></pre></div>


<div class="table-div">
<table border="1">
  <thead>
    <tr>
      <th></th>
      <th>PC1</th>
      <th>PC2</th>
      <th>PC3</th>
      <th>PC4</th>
      <th>PC5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6.626476</td>
      <td>-174.944631</td>
      <td>103.961564</td>
      <td>2.215012</td>
      <td>-9.158727</td>
    </tr>
    <tr>
      <th>1</th>
      <td>15.753434</td>
      <td>-107.886011</td>
      <td>89.561838</td>
      <td>22.584096</td>
      <td>-6.403647</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-7.291178</td>
      <td>-66.648771</td>
      <td>63.710631</td>
      <td>27.268419</td>
      <td>-4.361848</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-31.119019</td>
      <td>-22.354213</td>
      <td>53.012715</td>
      <td>37.152561</td>
      <td>4.960774</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.369718</td>
      <td>0.427355</td>
      <td>-0.015712</td>
      <td>0.438565</td>
      <td>-0.936091</td>
    </tr>
  </tbody>
</table>
</div>

<p><br></p>
<p><img alt="image alt text" src="https://gastonamengual.github.io/images/principal_component_analysis_3.png"></p>
<p><br></p>
<p>Finally, the previous Correlogram verifies how PCA has successfully transformed a correlated data matrix into a linearly independent data matrix, i.e. uncorrelated. The question of how many principal components to keep will highly depend on deciding what matters most, the size the matrix takes or the amount of variance explained, according to the requirements of the application.</p>
<h2>Note: Covariance and SVD differences</h2>
<div class="highlight"><pre><span></span><code><span class="n">covariance_components</span> <span class="o">=</span> <span class="n">principal_component_analysis_covariance</span><span class="p">(</span><span class="n">dataset</span><span class="p">)[</span><span class="s1">&#39;components&#39;</span><span class="p">]</span>
<span class="n">svd_components</span> <span class="o">=</span> <span class="n">principal_component_analysis_svd</span><span class="p">(</span><span class="n">dataset</span><span class="p">)[</span><span class="s1">&#39;components&#39;</span><span class="p">]</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">covariance_components</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">svd_components</span><span class="p">)))</span> <span class="o">==</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The absolute values of the components of both approaches match&#39;</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="err">The absolute values of the components of both approaches match</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="n">percentage_positive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">covariance_components</span><span class="p">,</span> <span class="n">svd_components</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">percentage_positive</span><span class="si">:</span><span class="s1">.3g</span><span class="si">}</span><span class="s1">% of the values of SVD are equally signed with Covariance values&#39;</span><span class="p">)</span>

<span class="n">percentage_positive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">covariance_components</span><span class="p">,</span> <span class="o">-</span><span class="n">svd_components</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">percentage_positive</span><span class="si">:</span><span class="s1">.3g</span><span class="si">}</span><span class="s1">% of the values of SVD have opposite sign than Covariance values&#39;</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="err">0.63% of the values of SVD are equally signed with Covariance values</span>
<span class="err">0.37% of the values of SVD have opposite sign than Covariance values</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">principal_component_analysis_covariance</span><span class="p">(</span><span class="n">dataset</span><span class="p">)[</span><span class="s1">&#39;explained_variance&#39;</span><span class="p">],</span> <span class="n">principal_component_analysis_svd</span><span class="p">(</span><span class="n">dataset</span><span class="p">)[</span><span class="s1">&#39;explained_variance&#39;</span><span class="p">]))</span> <span class="o">==</span> <span class="mi">1</span>

<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">principal_component_analysis_covariance</span><span class="p">(</span><span class="n">dataset</span><span class="p">)[</span><span class="s1">&#39;singular_values&#39;</span><span class="p">],</span> <span class="n">principal_component_analysis_svd</span><span class="p">(</span><span class="n">dataset</span><span class="p">)[</span><span class="s1">&#39;singular_values&#39;</span><span class="p">]))</span> <span class="o">==</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Both approaches have equal explained variance and singular values&#39;</span><span class="p">)</span>
</code></pre></div>


<div class="highlight"><pre><span></span><code><span class="err">Both approaches have equal explained variance and singular values</span>
</code></pre></div>


<p>As it can be observed, the principal components of both Covariance and SVD and covariance approaches differ in sign 37% of the times. No further analysis nor examination on the cause of this situation was performed.</p>
<hr>

<h1>Appendix: PCA and Clustering</h1>
<p>I found it extremely important not to abbreviate names, but to say the full words. That makes us remember what is <em>it</em> that we are discussing, and forces somehow a desire of understanding those words' meaning. Let's review the name of the central topic of this work: Principal Component Analysis. As discussed in the first section, "PCA transforms the data maximizing the variance over the projected points. Each new coordinate is called principal component". It follows that Principal Component Analysis is nothing more than the Analysis of the Principal Components, and Principal Components are new coordinates built by maximizing the variance. This article showed no visualization of the transformed data, as it most of the cases tempts to use Principal Component Analysis as a clustering technique (and sometimes clear clusters indeed can be seen). However, one must keep in mind that Principal Component Analysis is never trying to discern clusters, but instead it seeks to ANALYZE the PRINCIPAL COMPONENTS, which have nothing to do with clusters, but with keeping the maximum possible variability of the original data in the new coordinates.</p>
<p>The Principal-Component-Analysis-like algorithm for clustering is called Linear Discriminant Analysis. For more information of the difference between the two, Gopal Prasad Malakar made an excelent <a href="https://www.youtube.com/watch?v=M4HpyJHPYBY&amp;t=43s">video</a>, from which I borrow the following image to illustrate the case.</p>
<p><br></p>
<p><img src="https://i.stack.imgur.com/Tz5mA.png" style="width: 350px;"></p>
<h1>References</h1>
<p><a href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis" target="_blank">A step-by-step Explanation of Principal Component Analysis</a></p>
<p><a href="https://towardsdatascience.com/principal-component-analysis-pca-from-scratch-in-python-7f3e2a540c51" target="_blank">PCA From Scratch in Python</a></p>
<p><a href="https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/" target="_blank">How to Calculate Principal Component Analysis (PCA) from Scratch in Python</a></p>
<p><a href="https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/decomposition/pca.py#L385" target="_blank">Sklearn PCA Implementation</a></p>
<p><a href="https://statistics.laerd.com/spss-tutorials/principal-components-analysis-pca-using-spss-statistics.php" target="_blank">Assumptions</a></p>
<p><a href="https://www.youtube.com/watch?v=52d7ha-GdV8&amp;t=3s" target="_blank">PCA (Principal Component Analysis) in Python -  Python Engineer</a></p>
<p><a href="https://www.youtube.com/watch?v=FgakZw6K1QQ&amp;feature=emb_title" target="_blank">Principal Component Analysis (PCA), Step-by-Step - StatQuest</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </section>
      </div>

      <br><br>

    </article>


    <footer>
        <p>
          &copy;
          2021          Gastón Amengual
        </p>
    </footer>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-6KC62F9717"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());
  gtag('config', "G-6KC62F9717");</script>  </body>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: { inlineMath: [["$","$"],["\\(","\\)"]] },
    "HTML-CSS": {
      linebreaks: { automatic: true, width: "container" }
    }
    });
  </script>
</html>